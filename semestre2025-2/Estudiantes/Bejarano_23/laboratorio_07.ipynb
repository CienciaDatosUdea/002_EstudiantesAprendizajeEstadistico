{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "qMH6RprS5p6m"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Construir un clase  que permita definir una red neuronal con la topología\n",
        "deseada y la función de activación para cada capa, para ello deberá construir una funcion Topology con el número de capas de la red neuronal."
      ],
      "metadata": {
        "id": "U46GR1XR5N4A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se define la función de activación:"
      ],
      "metadata": {
        "id": "OpjupjqX59uE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def act_function(x, activation):\n",
        "  if activation == \"sigmoid\":\n",
        "    f = 1 / (1 + np.exp(-x))\n",
        "    fp = f * (1 - f)\n",
        "    return f, fp\n",
        "\n",
        "  elif activation == \"tanh\":\n",
        "    f = np.tanh(x)\n",
        "    fp = 1 - f**2\n",
        "    return f, fp\n",
        "\n",
        "  elif activation == \"relu\":\n",
        "    f = np.maximum(0, x)\n",
        "    fp = (x > 0).astype(float) # la derivada es 1 para x > 0, sino 0\n",
        "    return f, fp\n",
        "\n",
        "  elif activation is None or activation == \"None\":\n",
        "    # si no se especifíca la función de activación se usa la función identidad\n",
        "    return x, np.ones_like(x)\n",
        "\n",
        "  else:\n",
        "    raise ValueError(f\"Activation function '{activation}' not supported.\")"
      ],
      "metadata": {
        "id": "6UA3Tq9J5uz8"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se define la clase para una capa individual de la red:"
      ],
      "metadata": {
        "id": "RPWS4H_k6NHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class layer_nn():\n",
        "  def __init__(self, act_fun_name, nlayer_present, nlayer_before):\n",
        "    # se inicializan los parámetros de forma aleatoria entre -1 y 1\n",
        "    self.theta = 2 * np.random.random((nlayer_present, nlayer_before)) - 1\n",
        "    self.B = 2 * np.random.random((nlayer_present, 1)) - 1\n",
        "    self.act_fun_name = act_fun_name\n",
        "\n",
        "  # A_prev es el output de la anterior capa\n",
        "  def output(self, A_prev):\n",
        "    self.Z = np.dot(self.theta, A_prev) + self.B\n",
        "    self.A, _ = act_function(self.Z, self.act_fun_name)\n",
        "    return self.A, self.Z"
      ],
      "metadata": {
        "id": "trkw_uqD6Sy4"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Construir un generalizacion de la red, en el que entrada el valor inicial\n",
        "y la red neuronal completa arroje la salida y la actualizacion de la red con los parametros deseados:\n",
        "\n",
        "```python\n",
        "A, nn = forward_pass(A0, nn_red)\n",
        "```"
      ],
      "metadata": {
        "id": "o9pSdlDu_r-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se implementa la clase `NeuralNetwork`"
      ],
      "metadata": {
        "id": "Vh64axeNAroh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork():\n",
        "  def __init__(self, topology, activations):\n",
        "    self.layers = []\n",
        "    for i in range(1, len(topology)):\n",
        "      nlayer_present = topology[i]\n",
        "      nlayer_before = topology[i-1]\n",
        "      act_fun_name = activations[i]\n",
        "      self.layers.append(layer_nn(act_fun_name, nlayer_present, nlayer_before))\n",
        "\n",
        "  def forward_propagation(self, A0):\n",
        "    A_current = A0\n",
        "    for layer in self.layers:\n",
        "      A_current, _ = layer.output(A_current)\n",
        "    return A_current"
      ],
      "metadata": {
        "id": "nvdqJn2J_xB7"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se define la función `forward_pass`:"
      ],
      "metadata": {
        "id": "3seXvC1YA8DP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(A0, nn_red):\n",
        "  final_A = nn_red.forward_propagation(A0)\n",
        "  return final_A, nn_red"
      ],
      "metadata": {
        "id": "W6t54otBBA4N"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Encontrar la funcion de coste.\n",
        "\n",
        "\n",
        "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$"
      ],
      "metadata": {
        "id": "1LwTCrkYBfbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se define la función `compute_cost(A, Y)` que calcula la función de coste dada la salida predicha `A` y las etiquetas `Y`:"
      ],
      "metadata": {
        "id": "sy3JlMVvCDAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(A, Y):\n",
        "  m = Y.shape[1]  # número de ejemplos de entrenamiento\n",
        "\n",
        "  # Calcula el coste\n",
        "  cost = -(1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
        "\n",
        "  return cost"
      ],
      "metadata": {
        "id": "-vO1Hv0QBiS2"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Construir un codigo que permita realizar el BackwardPropagation."
      ],
      "metadata": {
        "id": "zJ3T2PH0C9jF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se añade el método `backward_propagation` a la clase `NeuralNetwork` que calcula gradientes y actualiza los pesos y bias en todas las capas:"
      ],
      "metadata": {
        "id": "ZkLcksx5QZQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork():\n",
        "  def __init__(self, topology, activations):\n",
        "    self.layers = []\n",
        "    self.activations = []\n",
        "    for i in range(1, len(topology)):\n",
        "      nlayer_present = topology[i]\n",
        "      nlayer_before = topology[i-1]\n",
        "      act_fun_name = activations[i]\n",
        "      self.layers.append(layer_nn(act_fun_name, nlayer_present, nlayer_before))\n",
        "\n",
        "  def forward_propagation(self, A0):\n",
        "    self.activations = [A0]\n",
        "    A_current = A0\n",
        "    for layer in self.layers:\n",
        "      A_current, Z_current = layer.output(A_current)\n",
        "      self.activations.append(A_current)\n",
        "    return A_current\n",
        "\n",
        "  def backward_propagation(self, X, Y, learning_rate):\n",
        "    final_A = self.forward_propagation(X)\n",
        "\n",
        "    m = X.shape[1]\n",
        "\n",
        "    # calculate dAL para la última capa\n",
        "    dAL = -(np.divide(Y, self.activations[-1]) - np.divide(1 - Y, 1 - self.activations[-1]))\n",
        "\n",
        "    current_dA = dAL\n",
        "\n",
        "    # se realiza el backward por las capas\n",
        "    # len(self.layers) es L (número de capas excluyendo el input)\n",
        "    # self.layer[l_idx] corresponde a la capa l_idx + 1\n",
        "    # self.activations[l_idx] corresponde a A^[l_idx]\n",
        "    for l_idx in reversed(range(len(self.layers))):\n",
        "      layer = self.layers[l_idx]\n",
        "\n",
        "      Z_l = layer.Z\n",
        "      A_prev_l = self.activations[l_idx] # A^[l] (activación desde la anterior capa)\n",
        "\n",
        "      # obtiene la derivada de la función de activación para la capa actual\n",
        "      _, fp = act_function(Z_l, layer.act_fun_name)\n",
        "\n",
        "      dZ_l = current_dA * fp\n",
        "\n",
        "      # calcula los gradientes para los pesos (dTheta) y los bias (dB)\n",
        "      dTheta_l = (1 / m) * np.dot(dZ_l, A_prev_l.T)\n",
        "      dB_l = (1 / m) * np.sum(dZ_l, axis=1, keepdims=True)\n",
        "\n",
        "      # actualiza los pesos y los bias\n",
        "      layer.theta -= learning_rate * dTheta_l\n",
        "      layer.B -= learning_rate * dB_l\n",
        "\n",
        "      # calcula dA para las anteriores capas para continuar el backpropagation\n",
        "      current_dA = np.dot(layer.theta.T, dZ_l)"
      ],
      "metadata": {
        "id": "Eoqpv3P_Qpgg"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probando la generalización de la NN"
      ],
      "metadata": {
        "id": "1X9InJu8SH4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topology = [2, 3, 1]\n",
        "activations = [None, 'relu', 'sigmoid']"
      ],
      "metadata": {
        "id": "RUxROCD1SRbi"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se crea una muesta de inputs X (2 características, 4 ejemplos):"
      ],
      "metadata": {
        "id": "Yah22SGtSZH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[0.6, 0.9, 0.7, 0.95],\n",
        "              [0.3, 0.6, 0.68, 0.09]])"
      ],
      "metadata": {
        "id": "j2owLmEvSYo2"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se crean las etiquetas Y (1 output, 4 ejemplos):"
      ],
      "metadata": {
        "id": "OUdbnt_ESn-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y = np.array([[0, 1, 1, 0]])"
      ],
      "metadata": {
        "id": "Q7ps6JrfSnZT"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se inicializa la red neuronal:"
      ],
      "metadata": {
        "id": "2myu4jjiSx0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NeuralNetwork(topology, activations)"
      ],
      "metadata": {
        "id": "0RJRaW5dS0hd"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.5\n",
        "epochs = 1000"
      ],
      "metadata": {
        "id": "RoncZN97S5Si"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  # forward propagation\n",
        "  A_final = nn.forward_propagation(X)\n",
        "\n",
        "  cost = compute_cost(A_final, Y)\n",
        "\n",
        "  # imprime el costo cada 100 épocas\n",
        "  if epoch % 100 == 0:\n",
        "    print(f\"Época {epoch}, Coste: {cost:.4f}\")\n",
        "\n",
        "  # backward propagation\n",
        "  nn.backward_propagation(X, Y, learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apak9e7oTBaZ",
        "outputId": "f1b9d71a-ce72-4669-c926-096a765e6972"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 0, Coste: 0.7748\n",
            "Época 100, Coste: 0.1062\n",
            "Época 200, Coste: 0.0196\n",
            "Época 300, Coste: 0.0089\n",
            "Época 400, Coste: 0.0055\n",
            "Época 500, Coste: 0.0038\n",
            "Época 600, Coste: 0.0029\n",
            "Época 700, Coste: 0.0023\n",
            "Época 800, Coste: 0.0019\n",
            "Época 900, Coste: 0.0016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Costo final después de {epochs} épocas: {cost}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iu3E_gVDTXcx",
        "outputId": "8308c493-0a1d-4594-97d2-45df7c52a278"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Costo final después de 1000 épocas: 0.001412394722058705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se muestra la predicción final:"
      ],
      "metadata": {
        "id": "OBmuSdTBTp3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_predictions = nn.forward_propagation(X)\n",
        "print(\"predicción final (A_final):\\n\", final_predictions)\n",
        "print(\"\\nEtiquetas (Y):\\n\", Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBRSPvJKTpda",
        "outputId": "bb4c9ff2-5b01-48cd-9cc4-204b640314a0"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicción final (A_final):\n",
            " [[2.99991419e-03 9.97695486e-01 9.99677802e-01 8.07066507e-06]]\n",
            "\n",
            "Etiquetas (Y):\n",
            " [[0 1 1 0]]\n"
          ]
        }
      ]
    }
  ]
}