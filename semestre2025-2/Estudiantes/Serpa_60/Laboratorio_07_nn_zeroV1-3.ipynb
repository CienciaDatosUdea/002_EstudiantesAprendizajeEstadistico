{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Jesus D Serpa P"
      ],
      "metadata": {
        "id": "2rO7lPQOwMsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topología de la red.\n",
        "\n",
        "1. Construir un clase  que permita definir una red neuronal con la topología\n",
        "deseada y la función de activación para cada capa, para ello deberá construir una funcion Topology con el número de capas de la red neuronal :\n",
        "\n",
        "Topology = [n_x, n_h1, n_h2, n_h3, ...,n_y]\n",
        "\n",
        "En este caso:\n",
        "- $n^{[0]}=n_x$ seran los valores de entradas de la capa de entrada\n",
        "- $n^{[1]}=n_{h1}$ Primera capa oculta de la red neuronal\n",
        "- $n^{[2]}=n_{h2}$ Segunda capa oculta de la red neuronal\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "- $n^{[l]}=n_{hl}$ Segunda capa oculta de la red neuronal\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "- $n^{[L]}=n_{y}$ Segunda capa oculta de la red neuronal\n",
        "\n",
        "donde\n",
        "\n",
        "- $\\mathrm{n_x}$: valores de entrada\n",
        "- $\\mathrm{n_{h1}}$: hidden layer 1\n",
        "- $\\mathrm{n_{h2}}$: hidden layer 2\n",
        "- $\\mathrm{n_y}$: last layer\n",
        "\n",
        "- $n^{[L]}=n_{y}$ Segunda capa oculta de la red neuronal\n",
        "\n",
        "\n",
        "También definir una lista con las funciones de activaciones para cada capa.\n",
        "\n",
        "\n",
        "activation=[None, relu, relu, relu, ...,sigmoid]\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "a. Cada unas de las capas deberá tener los parámetros de inicialización de manera aleatoria:\n",
        "\n",
        "\n",
        "La matriz de parametros para cada capa debera tener:\n",
        "\n",
        "\n",
        "$\\mathrm{dim(\\vec{b}^{[l]})}=n^{[l]}$\n",
        "\n",
        "$\\mathrm{dim(\\vec{\\Theta}^{[l]})}=n^{[l]}\\times n^{[l-1]}$\n",
        "\n",
        "Lo anteriores parametros deberán estar en el constructor de la clase.\n",
        "\n",
        "\n",
        "b. Construya un metodo llamado output cuya salida serán los valores de Z y A\n",
        "\n",
        "\n",
        "$\\mathrm{dim(\\vec{\\cal{A}}^{[l]})}=n^{[l-1]}\\times m $\n",
        "\n",
        "$\\mathrm{dim(\\vec{\\cal{Z}}^{[l]})}=n^{[l]}\\times m $."
      ],
      "metadata": {
        "id": "71MOc1VTwY60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class layer_nn:\n",
        "    def __init__(self, act_fun, nlayer_present, nlayer_before):\n",
        "        \"\"\"\n",
        "        act_fun: string con el nombre de la función de activación (\"sigmoid\", \"tanh\", \"relu\", ...)\n",
        "        nlayer_present: n^{[l]}  (número de neuronas de esta capa)\n",
        "        nlayer_before:  n^{[l-1]} (número de neuronas de la capa anterior)\n",
        "        \"\"\"\n",
        "        # Theta^{[l]} ~ n^{[l]} x n^{[l-1]}\n",
        "        self.theta = 2 * np.random.random((nlayer_present, nlayer_before)) - 1\n",
        "        # b^{[l]} ~ n^{[l]} x 1\n",
        "        self.b = 2 * np.random.random((nlayer_present, 1)) - 1\n",
        "\n",
        "        self.act_fun = act_fun\n",
        "\n",
        "        # Para almacenar resultados del forward:\n",
        "        self.Z = None   # Z^{[l]}\n",
        "        self.A = None   # A^{[l]}\n",
        "        self.dA = None  # derivada de activación f'(Z^{[l]}) (la usaremos en backprop)\n",
        "\n",
        "    def output(self, Z, A, dA):\n",
        "        \"\"\"\n",
        "        Guarda en la capa los valores de Z^{[l]}, A^{[l]} y f'(Z^{[l]})\n",
        "        \"\"\"\n",
        "        self.Z = Z\n",
        "        self.A = A\n",
        "        self.dA = dA\n"
      ],
      "metadata": {
        "id": "CoTSAdscwIKw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Construir un generalizacion de la red, en el que entrada el valor inicial\n",
        "y la red neuronal completa arroje la salida y la actualizacion de la red con los parametros deseados:\n",
        "\n",
        "  ```\n",
        "  A, nn = forward_pass(A0, nn_red)\n",
        "\n",
        " ```"
      ],
      "metadata": {
        "id": "6Srd5vDByX4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def act_function(Z, activation):\n",
        "    if activation == \"sigmoid\":\n",
        "        return 1 / (1 + np.exp(-Z))\n",
        "    elif activation == \"tanh\":\n",
        "        return np.tanh(Z)\n",
        "    elif activation == \"relu\":\n",
        "        return np.maximum(0, Z)\n",
        "    else:\n",
        "        # para la capa de entrada o identidad\n",
        "        return Z\n",
        "\n",
        "topology    = [12288, 3, 4, 6, 1]\n",
        "activations = [None, \"sigmoid\", \"sigmoid\", \"sigmoid\", \"sigmoid\"]\n",
        "\n",
        "nn_red = []\n",
        "for l in range(1, len(topology)):\n",
        "    layer = layer_nn(\n",
        "        act_fun=activations[l],\n",
        "        nlayer_present=topology[l],\n",
        "        nlayer_before=topology[l-1]\n",
        "    )\n",
        "    nn_red.append(layer)\n",
        "\n",
        "def forward_pass(A0, nn_red):\n",
        "    \"\"\"\n",
        "    A0     : matriz de entrada (n^{[0]} x m)\n",
        "    nn_red : lista de capas (objetos layer_nn)\n",
        "\n",
        "    Salida:\n",
        "      A  : salida de la última capa (n^{[L]} x m)\n",
        "      nn : red actualizada con Z^{[l]} y A^{[l]} guardados en cada capa\n",
        "    \"\"\"\n",
        "    A = A0\n",
        "    updated_nn = []\n",
        "\n",
        "    for layer in nn_red:\n",
        "        # Z^{[l]} = Θ^{[l]} A^{[l-1]} + b^{[l]}\n",
        "        Z = layer.theta @ A + layer.b     # (n^{[l]} x m)\n",
        "\n",
        "        # A^{[l]} = f(Z^{[l]})\n",
        "        A = act_function(Z, layer.act_fun)  # (n^{[l]} x m)\n",
        "\n",
        "        # Guardar Z^{[l]} y A^{[l]} en la capa\n",
        "        layer.output(Z, A, None)\n",
        "\n",
        "        updated_nn.append(layer)\n",
        "\n",
        "    return A, updated_nn"
      ],
      "metadata": {
        "id": "YEqLJ_mUwczX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m  = 5                     # número de ejemplos (solo para prueba)\n",
        "n_x = topology[0]          # tamaño de la entrada\n",
        "\n",
        "# Entrada A0 con la forma correcta (n_x x m)\n",
        "A0 = np.random.rand(n_x, m)\n",
        "\n",
        "# Forward de toda la red\n",
        "AL, nn = forward_pass(A0, nn_red)\n",
        "\n",
        "print(\"Forma de A0 (entrada):\", A0.shape)\n",
        "print(\"Forma de salida AL   :\", AL.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STcPwNNhyb3x",
        "outputId": "51f5229c-f399-4ee8-8bc7-af71bbad2c46"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forma de A0 (entrada): (12288, 5)\n",
            "Forma de salida AL   : (1, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Encontrar la funcion de coste.\n",
        "\n",
        "\n",
        "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "dGkPYU7y0I7T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(AL, Y):\n",
        "    \"\"\"\n",
        "    AL : salida de la última capa, de dimensión (1, m)\n",
        "    Y  : etiquetas verdaderas, de dimensión (1, m)\n",
        "\n",
        "    Devuelve:\n",
        "        cost : escalar con el valor de la función de coste\n",
        "    \"\"\"\n",
        "    m = Y.shape[1]          # número de ejemplos\n",
        "\n",
        "    # Evitar log(0)\n",
        "    eps = 1e-15\n",
        "    AL_clipped = np.clip(AL, eps, 1 - eps)\n",
        "\n",
        "    cost = - (1.0 / m) * np.sum(\n",
        "        Y * np.log(AL_clipped) + (1 - Y) * np.log(1 - AL_clipped)\n",
        "    )\n",
        "\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "zjjOo7hjy1RG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Construir un codigo que permita realizar el BackwardPropagation\n"
      ],
      "metadata": {
        "id": "au5bMTvu0NxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def act_derivative(Z, A, activation):\n",
        "    \"\"\"\n",
        "    Z : Z^{[l]}  (n^{[l]} x m)\n",
        "    A : A^{[l]}  (n^{[l]} x m)\n",
        "    \"\"\"\n",
        "    if activation == \"sigmoid\":\n",
        "        # f(z) = A  =>  f'(z) = A(1-A)\n",
        "        return A * (1 - A)\n",
        "    elif activation == \"tanh\":\n",
        "        # f(z) = tanh(z)  =>  f'(z) = 1 - A^2\n",
        "        return 1 - A**2\n",
        "    elif activation == \"relu\":\n",
        "        # f(z) = max(0,z)\n",
        "        return (Z > 0).astype(float)\n",
        "    else:\n",
        "        # identidad\n",
        "        return np.ones_like(Z)\n"
      ],
      "metadata": {
        "id": "sLxGO2hJ0Krw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_pass(AL, Y, nn_red, A0):\n",
        "    \"\"\"\n",
        "    AL    : salida de la última capa (n^{[L]} x m), normalmente (1 x m)\n",
        "    Y     : etiquetas verdaderas (1 x m)\n",
        "    nn_red: lista de capas (objetos layer_nn) de la red\n",
        "    A0    : entrada de la red (n^{[0]} x m)\n",
        "\n",
        "    Calcula y guarda en cada capa:\n",
        "        layer.dtheta  ~ dΘ^{[l]}\n",
        "        layer.db      ~ db^{[l]}\n",
        "\n",
        "    Devuelve:\n",
        "        nn_red actualizado con los gradientes.\n",
        "    \"\"\"\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # A^{[0]},...,A^{[L]}\n",
        "    A_caches = [A0] + [layer.A for layer in nn_red]   # len = L+1\n",
        "    L = len(nn_red)\n",
        "\n",
        "    #  Gradiente en la salida\n",
        "    # dJ/dAL (general, sin la simplificación A-Y)\n",
        "    eps = 1e-15\n",
        "    AL_clip = np.clip(AL, eps, 1 - eps)\n",
        "    dAL = -(np.divide(Y, AL_clip) - np.divide(1 - Y, 1 - AL_clip))\n",
        "\n",
        "    dA = dAL\n",
        "\n",
        "    # Recorrido hacia atrás\n",
        "\n",
        "    for l in reversed(range(L)):   # L-1, L-2, ..., 0\n",
        "        layer = nn_red[l]\n",
        "\n",
        "        Zl     = layer.Z          # Z^{[l+1]} en notación 0-index\n",
        "        Al     = layer.A          # A^{[l+1]}\n",
        "        A_prev = A_caches[l]      # A^{[l]}\n",
        "\n",
        "        # Derivada de la activación\n",
        "        g_prime = act_derivative(Zl, Al, layer.act_fun)\n",
        "\n",
        "        # dZ^{[l+1]} = dA^{[l+1]} ⊙ g'(Z^{[l+1]})\n",
        "        dZ = dA * g_prime                           # (n^{[l+1]} x m)\n",
        "\n",
        "        # dΘ^{[l+1]} = (1/m) dZ^{[l+1]} A^{[l]T}\n",
        "        dtheta = (1.0 / m) * (dZ @ A_prev.T)        # (n^{[l+1]} x n^{[l]})\n",
        "\n",
        "        # db^{[l+1]} = (1/m) sum_j dZ^{[l+1]}_j\n",
        "        db = (1.0 / m) * np.sum(dZ, axis=1, keepdims=True)  # (n^{[l+1]} x 1)\n",
        "\n",
        "        # dA^{[l]} = Θ^{[l+1]T} dZ^{[l+1]}  (para la capa anterior)\n",
        "        dA = layer.theta.T @ dZ                     # (n^{[l]} x m)\n",
        "\n",
        "        # Guardar gradientes en la capa\n",
        "        layer.dtheta = dtheta\n",
        "        layer.db     = db\n",
        "\n",
        "    return nn_red\n"
      ],
      "metadata": {
        "id": "4-384Ecw0vYE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AL, nn = forward_pass(A0, nn_red)   # AL: salida (1 x m)\n",
        "\n",
        "# Define Y (true labels) with the correct shape (1, m)\n",
        "m = AL.shape[1]  # Get m from the shape of AL\n",
        "Y = np.random.randint(0, 2, size=(1, m)) # Example: binary labels for m samples\n",
        "\n",
        "cost = cost_function(AL, Y)         # del punto 3\n",
        "nn    = backward_pass(AL, Y, nn, A0)\n",
        "\n"
      ],
      "metadata": {
        "id": "okzLX0Fc06sJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_params(nn_red, alpha):\n",
        "    \"\"\"\n",
        "    Actualiza los parámetros de la red usando los gradientes\n",
        "    calculados en backward_pass.\n",
        "    \"\"\"\n",
        "    for layer in nn_red:\n",
        "        layer.theta = layer.theta - alpha * layer.dtheta\n",
        "        layer.b     = layer.b     - alpha * layer.db\n"
      ],
      "metadata": {
        "id": "m7PQjH2v07W-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward\n",
        "AL, nn = forward_pass(A0, nn_red)\n",
        "\n",
        "# Costo (punto 3)\n",
        "J = cost_function(AL, Y)\n",
        "\n",
        "# BackwardPropagation (punto 4)\n",
        "nn = backward_pass(AL, Y, nn, A0)\n",
        "\n",
        "# Actualización de parámetros (gradiente descendente)\n",
        "alpha = 0.01\n",
        "update_params(nn, alpha)\n"
      ],
      "metadata": {
        "id": "ZpkpGy5E1z0R"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aXyGRg9X1003"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}