{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Salomon Uran Parra C.C. 1015068767**"
      ],
      "metadata": {
        "id": "VaK8mVtprX2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Laboratorio 9 - Aprendizaje Estadistico**"
      ],
      "metadata": {
        "id": "XndpJMsrrYvx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoMQF79PrAsO"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import graphviz\n",
        "#from sklearn.tree import export_graphviz\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import tree\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Importar iris dataset**\n"
      ],
      "metadata": {
        "id": "FfwW1He0rd9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#vamos a importar el iris dataset\n",
        "iris = load_iris()"
      ],
      "metadata": {
        "id": "k-5g_hzQrgTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Generar una intuición respecto a iris dataset. Ver laboratorio anterior.**"
      ],
      "metadata": {
        "id": "yuIfg7parkci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora usando la descripcion del dataset iris, se puede saber que el dataset esta compuesto por 150 registros de 3 clases distintas de plantas iris (50 de Setosa, de Versicolour y de Virginica). En cada registro se encuentran 4 atributos numericos de la planta, que son la longitud y ancho del sepalo, y longitud y ancho del petalo, todo en centimetros."
      ],
      "metadata": {
        "id": "JrZFmg7btuc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(iris.DESCR)"
      ],
      "metadata": {
        "id": "X3pQ9zpZrnhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Dividir el *dataset* empleanto el metodo train_test_split de sklearn**\n"
      ],
      "metadata": {
        "id": "ZzBtBJCErpRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#se hace un split para datos de entrenamiento y test usando un 33% para test, esto significa 100 datos de entrenamiento\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "G5KPx8E5r2Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Dentro de sklearn, aplicar el algoritmo DecisionTreeClassifier.**"
      ],
      "metadata": {
        "id": "3Wn8Nhblr4wV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#usando un decision tree con una limitacion de un maximo de profundidad de 100 en el arbol, para evitar overfitting\n",
        "clf = DecisionTreeClassifier(max_depth=100)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(f\"{clf.score(X_test, y_test)} \")\n",
        "print(f\"{clf.score(X_train, y_train)}\" )"
      ],
      "metadata": {
        "id": "MB7ceGuzr9Mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Para el clasificador analice el gráfico de desición:**\n",
        "\n",
        "```\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
        "\n",
        "  fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300)\n",
        "  graph=tree.plot_tree(clf,class_names = iris.target_names,\\\n",
        "                feature_names = iris.feature_names,\\\n",
        "                impurity=False, filled=True,rounded=True )\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "BU5gY536r_D2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=300)\n",
        "graph=tree.plot_tree(clf,class_names = iris.target_names, feature_names = iris.feature_names, impurity=False, filled=True,rounded=True )"
      ],
      "metadata": {
        "id": "32mFBGaRsEiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al graficar el plot_tree, se emplean colores cuando se puede distinguir la clase mayoritaria en un nodo, y cuando el nodo es blanco, indica una decision o condicional en el arbol en donde este se bifurca. Del grafico podemos ver que el arbol inicia evaluando la condicion del ancho del petalo menor a 0.8 cm, en donde logra bifurcar los datos entre 31 setosas (que cumplen la condicion) y el restante que son 69. Luego decide nuevamente con una condicion mas relajada del ancho del petalo menor a 1.75 cm, donde logra nuevamente separar entre una mayoria de Versicolor que cumplen y otra mayoria de Virginica que no. De esta forma continua evaluando multiples condiciones sobre el sepalo tambien para terminar de separar completamente el dataset, logrando diferenciar entre las 3 clases presentes en el."
      ],
      "metadata": {
        "id": "hCIlvX72woJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Características importantes.**\n",
        "\n",
        "```\n",
        "clf.feature_importances_\n",
        "caract = iris.data.shape[1]\n",
        "plt.barh(range(caract), clf.feature_importances_)\n",
        "plt.yticks(np.arange(caract),iris.feature_names)\n",
        "plt.xlabel('Importancia de las características')\n",
        "plt.ylabel('Características')\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "xmErMOvysIBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Empleando el feature_importances_ del classifier, podemos ver que el feature con mayor peso o importancia en el decision tree es el ancho del petalo, que esta en concordancia con el punto anterior, pues el arbol logra separar la mayoria de datos de cada una de las clases (en los primeros dos nodos) evaluando unicamente condiciones sobre este feature. De este modo, parece que el ancho del petalo es el feature que mayor diferenciacion arroja entre las 3 clases."
      ],
      "metadata": {
        "id": "HS0PqnDN0LFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf.feature_importances_\n",
        "caract = iris.data.shape[1]\n",
        "plt.barh(range(caract), clf.feature_importances_)\n",
        "plt.yticks(np.arange(caract),iris.feature_names)\n",
        "plt.xlabel('Importancia de las características')\n",
        "plt.ylabel('Características')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HFLHscC-sOv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Fronteras de desición.**\n",
        "\n"
      ],
      "metadata": {
        "id": "MIJnRA0usQ9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora por ultimo, se puede graficar la frontera de decision que se genera entre un par de caracteristicas distintas de las plantas usando el decision tree classifier. Entre par de caracteristicas, vemos por ejemplo que el ancho del petalo versus el ancho del sepalo arrojan tres regiones bien definidas para las tres clases. Ademas, aunque el decision tree igualmente es capaz de clasificar los puntos, hay features que arrojan fronteras mucho mas complejas, como por ejemplo el ancho versus la longitud de los sepalos. De igual forma, a pesar de la complejidad de las fronteras, vemos que el decision tree clasifica bien, sin embargo, se debe analizar posibilidades de overfitting."
      ],
      "metadata": {
        "id": "HA20KkGe18hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = 3\n",
        "plot_colors = \"bry\"\n",
        "plot_step = 0.02\n",
        "iris = load_iris()\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],\n",
        "                                [1, 2], [1, 3], [2, 3]]):\n",
        "    # We only take the two corresponding features\n",
        "    X = iris.data[:, pair]\n",
        "    y = iris.target\n",
        "\n",
        "    # Shuffle\n",
        "    idx = np.arange(X.shape[0])\n",
        "    np.random.seed(13)\n",
        "    np.random.shuffle(idx)\n",
        "    X = X[idx]\n",
        "    y = y[idx]\n",
        "\n",
        "    # Standardize\n",
        "    mean = X.mean(axis=0)\n",
        "    std = X.std(axis=0)\n",
        "    X = (X - mean) / std\n",
        "\n",
        "    # Train\n",
        "    clf = DecisionTreeClassifier().fit(X, y)\n",
        "\n",
        "    # Plot the decision boundary\n",
        "    plt.subplot(2, 3, pairidx + 1)\n",
        "\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
        "                         np.arange(y_min, y_max, plot_step))\n",
        "\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "    plt.xlabel(iris.feature_names[pair[0]])\n",
        "    plt.ylabel(iris.feature_names[pair[1]])\n",
        "    plt.axis(\"tight\")\n",
        "\n",
        "    # Plot the training points\n",
        "    for i, color in zip(range(n_classes), plot_colors):\n",
        "        idx = np.where(y == i)\n",
        "        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n",
        "                    cmap=plt.cm.Paired)\n",
        "\n",
        "    plt.axis(\"tight\")\n",
        "\n",
        "plt.suptitle(\"Decision surface of a decision tree using paired features\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DyrGPqI2sYad"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}