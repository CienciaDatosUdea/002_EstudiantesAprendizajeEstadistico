{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Salomon Uran Parra C.C. 1015068767**"
      ],
      "metadata": {
        "id": "rUfjCcuwQT3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Laboratorio 7 - Aprendizaje Estadistico**"
      ],
      "metadata": {
        "id": "pBf12BrLQUtg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUC-947ZQSBy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import scipy as sc\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pylab as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#solo se ejecuta una vez para cargar los archivos, luego se comenta\n",
        "\n",
        "!wget https://github.com/CienciaDatosUdea/002_EstudiantesAprendizajeEstadistico/raw/f170fa0980192f9fe2ee5f2d7bd7962b4d66d517/semestre2025-2/Laboratorios/dataset/train_catvnoncat.h5\n",
        "!wget https://github.com/CienciaDatosUdea/002_EstudiantesAprendizajeEstadistico/raw/f170fa0980192f9fe2ee5f2d7bd7962b4d66d517/semestre2025-2/Laboratorios/dataset/test_catvnoncat.h5\n"
      ],
      "metadata": {
        "id": "mW36-s7xQXrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train= \"train_catvnoncat.h5\"\n",
        "train_dataset = h5py.File(data_train, \"r\")\n",
        "\n",
        "data_test= \"test_catvnoncat.h5\"\n",
        "test_dataset = h5py.File(data_test, \"r\")\n",
        "\n",
        "# Read the data\n",
        "xtrain_classes, xtrain, train_label =\\\n",
        "train_dataset[\"list_classes\"],train_dataset[\"train_set_x\"],train_dataset[\"train_set_y\"]\n",
        "\n",
        "test_classes, xtest,test_label =\\\n",
        "test_dataset[\"list_classes\"],test_dataset[\"test_set_x\"],test_dataset[\"test_set_y\"]\n",
        "\n",
        "\n",
        "xtrain_= np.reshape(xtrain,( 64*64*3 , 209))/255\n",
        "xtest_ = np.reshape(xtest, ( 64*64*3 , 50))/255\n",
        "\n",
        "ytrain_ = np.reshape(train_label,(1,209))\n",
        "ytest_ = np.reshape(test_label,(1,50))\n",
        "\n",
        "print(xtrain_.shape)\n",
        "print(xtest_.shape)\n",
        "print(ytrain_.shape)\n",
        "print(ytest_.shape)"
      ],
      "metadata": {
        "id": "R06LUBeuQbIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topología de la red.\n",
        "\n",
        "1. Construir un clase  que permita definir una red neuronal con la topología\n",
        "deseada y la función de activación para cada capa, para ello deberá construir una funcion Topology con el número de capas de la red neuronal :\n",
        "\n",
        "Topology = [n_x, n_h1, n_h2, n_h3, ...,n_y]\n",
        "\n",
        "En este caso:\n",
        "- $n^{[0]}=n_x$ seran los valores de entradas de la capa de entrada\n",
        "- $n^{[1]}=n_{h1}$ Primera capa oculta de la red neuronal\n",
        "- $n^{[2]}=n_{h2}$ Segunda capa oculta de la red neuronal\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "- $n^{[l]}=n_{hl}$ Segunda capa oculta de la red neuronal\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "- $n^{[L]}=n_{y}$ Segunda capa oculta de la red neuronal\n",
        "\n",
        "donde\n",
        "\n",
        "- $\\mathrm{n_x}$: valores de entrada\n",
        "- $\\mathrm{n_{h1}}$: hidden layer 1\n",
        "- $\\mathrm{n_{h2}}$: hidden layer 2\n",
        "- $\\mathrm{n_y}$: last layer\n",
        "\n",
        "- $n^{[L]}=n_{y}$ Segunda capa oculta de la red neuronal\n",
        "\n",
        "\n",
        "También definir una lista con las funciones de activaciones para cada capa.\n",
        "\n",
        "\n",
        "activation=[None, relu, relu, relu, ...,sigmoid]\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "a. Cada unas de las capas deberá tener los parámetros de inicialización de manera aleatoria:\n",
        "\n",
        "\n",
        "La matriz de parametros para cada capa debera tener:\n",
        "\n",
        "\n",
        "$\\mathrm{dim(\\vec{b}^{[l]})}=n^{[l]}$\n",
        "\n",
        "$\\mathrm{dim(\\vec{\\Theta}^{[l]})}=n^{[l]}\\times n^{[l-1]}$\n",
        "\n",
        "Lo anteriores parametros deberán estar en el constructor de la clase.\n",
        "\n",
        "\n",
        "b. Construya un metodo llamado output cuya salida serán los valores de Z y A\n",
        "\n",
        "\n",
        "$\\mathrm{dim(\\vec{\\cal{A}}^{[l]})}=n^{[l-1]}\\times m $\n",
        "\n",
        "$\\mathrm{dim(\\vec{\\cal{Z}}^{[l]})}=n^{[l]}\\times m $.\n",
        "\n",
        "2. Construir un generalizacion de la red, en el que entrada el valor inicial\n",
        "y la red neuronal completa arroje la salida y la actualizacion de la red con los parametros deseados:\n",
        "\n",
        "  ```\n",
        "  A, nn = forward_pass(A0, nn_red)\n",
        "\n",
        " ```\n",
        "3. Encontrar la funcion de coste.\n",
        "\n",
        "\n",
        "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n",
        "\n",
        "\n",
        "4. Construir un codigo que permita realizar el BackwardPropagation"
      ],
      "metadata": {
        "id": "5LOgrbJDQeip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def act_funcion(x, activation):\n",
        "    if activation==\"sigmoid\":\n",
        "      x_arr = np.asarray(x)\n",
        "      f = 1/(1+np.exp(-x_arr))\n",
        "      fp = f*(1-f)\n",
        "      return np.array([f, fp])\n",
        "      #creacion de la funcion sigmoide\n",
        "\n",
        "    elif activation==\"relu\":\n",
        "      x_arr = np.asarray(x)\n",
        "      f = np.maximum(0, x_arr)\n",
        "      fp = (x_arr > 0).astype(float)\n",
        "      return np.array([f, fp])\n",
        "      #creacion de la funcion relu\n",
        "\n",
        "    elif activation is None or activation==\"linear\":\n",
        "      x_arr = np.asarray(x)\n",
        "      return np.array([x_arr, np.ones_like(x_arr)])\n",
        "      #activacion lineal, osea no aplicar ninguna funcion\n",
        "\n",
        "class layer_nn():\n",
        "  def __init__(self, n_pr, n_be, activation):\n",
        "    self.theta = 2*np.random.random((n_pr, n_be)) - 1\n",
        "    self.b = 2*np.random.random((n_pr, 1))-1\n",
        "    self.activation = activation\n",
        "    self.dtheta = None\n",
        "    self.db = None\n",
        "    #se crean las caracteristicas dtheta y db para guardar los gradientes y actualizar luego los parametros\n",
        "\n",
        "  def output(self, X): #aqui X es un input de dimension nb x m, es el output del layer previo\n",
        "    self.Z = self.theta @ X + self.b #sirve aun si theta @ x es npr x m, va a devolver un npr x m con b sumado a cada columna\n",
        "    self.A = act_funcion(self.Z, self.activation)[0] #devuelve tambien un npr x m con la funcion de activacion aplicada a cada elemento\n",
        "    self.Ap = act_funcion(self.Z, self.activation)[1] #devuelve la derivada de la funcion de activacion, npr x m\n",
        "\n",
        "  def update(self, learning_rate):\n",
        "    self.theta = self.theta - learning_rate * self.dtheta\n",
        "    self.b = self.b - learning_rate * self.db\n",
        "    #actualiza los parametros de la capa dado un learning_rate\n",
        "\n",
        "def cost_func(Y, A): #se le ingresara el label del target y el output del forward, A debe ser un array, Y ingresa como 1 x m y A tambien 1 x m,\n",
        "    m = Y.shape[1]\n",
        "    Y = np.array(Y)\n",
        "    A = np.array(A)\n",
        "\n",
        "    epsilon = 1e-10\n",
        "    A = np.clip(A, epsilon, 1 - epsilon)\n",
        "    #en caso de que A contenga 0s o 1s, los reemplaza por epsilon o 1-epsilon, para evitar problemas con el logaritmo\n",
        "\n",
        "    ji = - ( Y*np.log(A) + (1-Y)*np.log(1-A)) #funcion de coste\n",
        "    cost = ji.sum()\n",
        "    return cost/m\n",
        "\n",
        "class model_nn(): #la clase se define con una topologia y una lista de funciones de activacion para cada capa\n",
        "    def __init__(self, topology, activation):\n",
        "        self.topology = topology\n",
        "        self.layers = []\n",
        "        self.activation = activation\n",
        "\n",
        "    def Topology(self): #construye la arquitectura de la red, osea capas y numero de neuronas por capa, ademas de su funcion de activacion\n",
        "        for i in range(len(self.topology)-1):\n",
        "            self.layers.append(layer_nn(self.topology[i+1],self.topology[i],self.activation[i]))\n",
        "\n",
        "    def forward(self, X): #para un dato de entrada n0 x m (un dato de entrenamiento), hace el forward en los layers guardados\n",
        "        self.layers[0].output(X)\n",
        "        for i in range(len(self.layers)-1):\n",
        "            self.layers[i+1].output(self.layers[i].A)\n",
        "        return self.layers[-1].A #retorna una salida 1 x m\n",
        "\n",
        "    def backward(self, X, Y, alpha = 0.01):\n",
        "      #nuevamente, aqui X es una entrada n0 x m (input de entrenamiento) y Y es un dato 1xm que es el label del target, haremos un backward\n",
        "\n",
        "        self.A_L = self.forward(X) #debe ser una salida 1 x m, un forward para cada dato de entrenamiento con n0 features\n",
        "\n",
        "        self.cost = cost_func(Y, self.A_L) #funcion de coste para el forward realizado\n",
        "\n",
        "        m = Y.shape[1] #dimension o numero de datos de entrenamiento\n",
        "\n",
        "        L = len(self.layers) #numero de capas en la red\n",
        "\n",
        "        epsilon = 1e-10\n",
        "        self.A_L = np.clip(self.A_L, epsilon, 1 - epsilon) #evita problemas de division por cero en el dA_L\n",
        "\n",
        "        dA_L = -(np.divide(Y, self.A_L) - np.divide(1 - Y, 1 - self.A_L)) #dA_L como esta definido\n",
        "\n",
        "        fp_L = self.layers[L-1].Ap #derivada de la funcion de activacion en la ultima capa\n",
        "\n",
        "        dZ_L = np.multiply(dA_L,fp_L) #dZ de la ultima capa\n",
        "\n",
        "\n",
        "        if L == 1: #en caso de que haya una unica capa, el output de la capa anterior es X\n",
        "            self.layers[L-1].dtheta = dZ_L @ X.T / m\n",
        "        else: #output de la capa anterior\n",
        "            self.layers[L-1].dtheta = dZ_L @ self.layers[L-2].A.T / m\n",
        "            #son los gradientes para los parametros theta de la ultima capa\n",
        "\n",
        "        self.layers[L-1].db = np.sum(dZ_L, axis=1, keepdims = True) / m #gradiente para los parametros b de la ultima capa\n",
        "\n",
        "        for l in range(L-2, -1, -1): #se itera desde la penultima capa hasta la primera, al reves, backpropagation\n",
        "\n",
        "\n",
        "            dA = np.dot(self.layers[l+1].theta.T, dZ_L) #dA de la capa l empleando los thetas de la capa l+1 y el dZ de la misma capa\n",
        "\n",
        "            fp = self.layers[l].Ap #derivada de la funcion de activacion en la capa l\n",
        "            dZ_L = np.multiply(dA, fp) # dZ para la capa l-esima\n",
        "\n",
        "            if l == 0: #para la primera capa, el input es X\n",
        "                self.layers[l].dtheta = dZ_L @ X.T / m\n",
        "            else: #para las otras capas, se toma el output de la capa l-1\n",
        "                self.layers[l].dtheta = dZ_L @ self.layers[l-1].A.T / m\n",
        "                #gradiente los thetas en la capa l-esima\n",
        "\n",
        "            self.layers[l].db = np.sum(dZ_L, axis=1,keepdims = True) / m #gradiente de los b en la capa l\n",
        "\n",
        "        for l in range(L):\n",
        "            self.layers[l].update(alpha)\n",
        "            #actualizacion de los parametros en todas las capas dado un learning rate alpha"
      ],
      "metadata": {
        "id": "vJgKNzNWQcWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, una vez construida la clase del modelo de red neuronal, podemos construir la topologia de una red para hacer un clasificador de gatos empleando el dataset definido mas arriba:"
      ],
      "metadata": {
        "id": "KdIQZTguV5iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_x = xtrain_.shape[0]\n",
        "n_y = ytrain_.shape[0]\n",
        "\n",
        "np.random.seed(1) #para tener reproducibilidad\n",
        "\n",
        "topology = [n_x,  20, 15, 10,  n_y] #se define la topologia con el numero de inputs, de neuronas por capa oculta y de outputs\n",
        "\n",
        "activations = ['relu', 'relu', 'relu', 'sigmoid'] #se definen las funciones de activacion para cada capa, relu para las ocultas, sigmoide para el output\n",
        "\n",
        "mdl = model_nn(topology, activations) #se crea el modelo\n",
        "\n",
        "mdl.Topology() #se crea la topologia\n",
        "\n",
        "alpha = 0.0045 #learning rate\n",
        "num_epochs = 3000 #numero de epocas"
      ],
      "metadata": {
        "id": "lX3bZzAG0ep-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Asi pues, ensayaremos entrenar una red neuronal con el dataset xtrain con 3 capas ocultas y una capa de salida. Las capas intermedias tienen 20, 15 y 10 neuronas respectivamente con una funcion de activacion ReLu. La ultima capa sale con una funcion de activacion sigmoide. El modelo se entrenara con un learning rate de $\\alpha = 0.0045$ para todas las capas y se realizaran un total de 3000 epocas de entrenamiento."
      ],
      "metadata": {
        "id": "chNaiLbgCUDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#para las siguientes celdas de codigo, se empleo ayuda de la IA Gemini para la sintaxis y orden de la escritura\n",
        "#la documentacion y ciertas correciones de funcionamiento del codigo se hicieron a mano\n",
        "\n",
        "def calculate_accuracy(predictions, true_labels): #se le entrega la prediccion y el label verdadero (target)\n",
        "    #funcion que permite calcular la precision de una prediccion porcentualmente\n",
        "    binary_predictions = (predictions > 0.5).astype(float)\n",
        "    accuracy = np.mean(binary_predictions == true_labels) * 100\n",
        "    return accuracy\n",
        "\n",
        "costs = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    mdl.backward(xtrain_, ytrain_, alpha) #para la epoca actual, hacemos el forward, el backward y actualizamos los parametros de la red\n",
        "    costs.append(mdl.cost) #guardamos la funcion de coste de la epoca actual\n",
        "\n",
        "    train_predictions = mdl.forward(xtrain_)\n",
        "    train_acc = calculate_accuracy(train_predictions, ytrain_)\n",
        "    train_accuracies.append(train_acc)\n",
        "    #se calcula la precision de la prediccion en el set de entrenamiento\n",
        "\n",
        "    test_predictions = mdl.forward(xtest_)\n",
        "    test_acc = calculate_accuracy(test_predictions, ytest_)\n",
        "    test_accuracies.append(test_acc)\n",
        "    #se calcula la precision de la prediccion en el set de test\n",
        "\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Epoch {i}: Cost = {mdl.cost:.4f}, Train Accuracy = {train_acc:.2f}%, Test Accuracy = {test_acc:.2f}%\")\n",
        "        #se imprime la precision en el entrenamiento y test por cada epoca\n",
        "\n",
        "print(f\"Training complete. Final Cost: {mdl.cost:.4f}\")\n",
        "print(f\"Final Training Accuracy: {train_accuracies[-1]:.2f}%\")\n",
        "print(f\"Final Test Accuracy: {test_accuracies[-1]:.2f}%\")"
      ],
      "metadata": {
        "id": "-70LEK-6AJ02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En general, dependiendo de la semilla que se use para inicializar los pesos de la red, vemos en estas 3000 primeras epocas, el modelo tiende a un overfitting sobre los datos de entrenamiento, mientras que tiene fuertes dificultades para generalizar sobre los datos de test. Es decir, el modelo se esta \"aprendiendo de memoria\" los datos de entrenamiento."
      ],
      "metadata": {
        "id": "KURUO-dnEqLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "ax[0].plot(costs)\n",
        "ax[0].set_xlabel(\"Epoch\")\n",
        "ax[0].set_ylabel(\"Cost\")\n",
        "ax[0].set_title(\"Learning Curve (Cost)\")\n",
        "ax[0].grid(True)\n",
        "\n",
        "ax[1].plot(train_accuracies, label='Train Accuracy')\n",
        "ax[1].plot(test_accuracies, label='Test Accuracy')\n",
        "ax[1].set_xlabel(\"Epoch\")\n",
        "ax[1].set_ylabel(\"Accuracy (%)\")\n",
        "ax[1].set_title(\"Learning Curve (Accuracy)\")\n",
        "ax[1].legend()\n",
        "ax[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "77AD6hdZETAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui podemos observar la evolucion tanto de la funcion de coste como de las graficas de aprendizaje del modelo. En el dataset de test, se queda estancado y practicamente adivina el resultado. En el dataset de entrenamiento lo va aprendiendo poco a poco, claro sintoma de overfitting."
      ],
      "metadata": {
        "id": "_8bPIWs_FFJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(xtrain[14])"
      ],
      "metadata": {
        "id": "bVKZp2JSsHWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_original = xtrain_[:, 14]\n",
        "image_reshaped = image_original.reshape(n_x, 1)\n",
        "\n",
        "prediction = mdl.forward(image_reshaped)\n",
        "print(f\"Prediccion de la imagen: {prediction}\")\n",
        "\n",
        "binary_prediction = (prediction > 0.5).astype(int)\n",
        "print(f\"Categoria en la que entra la clasificacion: {binary_prediction}\")\n"
      ],
      "metadata": {
        "id": "J8TbqF-StGbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(xtrain[15])"
      ],
      "metadata": {
        "id": "Sk3QfG7oFyfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_original = xtrain_[:, 15]\n",
        "image_reshaped = image_original.reshape(n_x, 1)\n",
        "\n",
        "prediction = mdl.forward(image_reshaped)\n",
        "print(f\"Prediccion de la imagen: {prediction}\")\n",
        "\n",
        "binary_prediction = (prediction > 0.5).astype(int)\n",
        "print(f\"Categoria en la que entra la clasificacion: {binary_prediction}\")\n"
      ],
      "metadata": {
        "id": "yrkPIGU5F0f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En estas ultimas celdas, vemos la prediccion que lanza sobre una imagen que claramente es un gato y sobre otra que claramente no lo es. Esto muestra que esta aprendiendo bien los datos de entrenamiento, pero luego al observar el conjunto de test, sabemos que lo que esta haciendo es un overfitting. Habria que explorar tecnicas o topologias de la red que le permitan una mayor libertad de los parametros para generalizar mejor el modelo y poder performar mejor en los test. Recordar que la categoria 1 es para los gatos y 0 es para los no gatos."
      ],
      "metadata": {
        "id": "zcu7uRoTFcDD"
      }
    }
  ]
}