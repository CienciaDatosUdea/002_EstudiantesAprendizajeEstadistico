{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Salomon Uran Parra C.C. 1015068767**"
      ],
      "metadata": {
        "id": "W0rqZPSs-0P3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Laboratorio 5 - Aprendizaje Estadistico**"
      ],
      "metadata": {
        "id": "gQ-66cg8-3ru"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwAXto3q-wQ-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **0. Demostrar que:**\n",
        "\n",
        "- $J(\\theta_1,\\theta_2,\\theta_3, ...,\\theta_n ) = \\frac{1}{2m} (\\Theta ^ T X - y^T) (\\Theta ^ T X - y^T)^T$\n",
        "\n",
        "- $J= \\frac{1}{2m} ((\\Theta ^T X) (\\Theta ^T X)^T - 2(\\Theta ^T X)Y  + Y^TY $)\n",
        "\n",
        "\n",
        "- $ \\nabla _{\\theta} J = \\frac{1}{m} ( X(X^T \\Theta) -XY)$\n",
        "\n",
        "\n",
        "Para encontrar el valor minimo de \\theta,  $\\nabla _{\\theta} J = 0$,\n",
        "\n",
        "- $\\Theta = (X^T X)^{-1} X^T y$"
      ],
      "metadata": {
        "id": "1KDqcV6h-7GX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solución**\n",
        "\n",
        "Empecemos primero por la definición del modelo multilineal. Supongamos que $\\hat{Y}$ es una variable (target) que decimos, depende de $n$ features (que también son variables) $\\hat{X} ^{(i)}$ con $i = 1, 2, \\cdots ,n$. Vamos a definir un modelo de predicción que es lineal en cada uno de los $n$ features y que depende de $n+1$ parámetros $\\theta_j$ con $j = 0,1,\\cdots, n$, de tal forma que la predicción en un valor $l$ de los features tiene la siguiente forma:\n",
        "\n",
        "$$h(\\theta,X_l) = \\theta_0 + \\theta_1 X^{(1)}_l + \\cdots + \\theta_n X^{(n)}_l$$\n",
        "\n",
        "Si para este valor $l$ de los features hay asociado un valor $Y_l$ de la variable target, podemos usar una métrica euclídea $||\\cdot||$ para cuantizar de alguna forma que tan cercana o alejada es la predicción $h(\\theta,X_l)$ respecto a $Y_l$ para un conjunto dado de parámetros $\\theta = (\\theta_0, \\theta_1, \\cdots, \\theta_n)$. Si con esta métrica empleamos un error cuadrático medio sobre una cantidad de $m$ conjuntos de valores disponibles tanto para los features como para la variable target, podemos definir una función llamada la función coste $J(\\theta)$ de la siguiente forma:\n",
        "\n",
        "$$J(\\theta) = \\frac{1}{2m}\\sum_{i = 1}^m (\\theta_0 + \\theta_1 X^{(1)}_i + \\cdots + \\theta_n X^{(n)}_i - Y_i)^2$$\n",
        "\n",
        "Es fácil ver el carácter vectorial de la expresión anterior. Si se definen los siguientes vectores $Y = (Y_1 , \\cdots, Y_m) \\in \\mathbb{R}^{m\\times 1}$ y $\\Theta = (\\theta_0, \\theta_1 , \\cdots, \\theta_n) \\in \\mathbb{R}^{(n+1)\\times 1}$, y la matriz:\n",
        "\n",
        "\\begin{equation}\n",
        "X =\n",
        "\\begin{bmatrix}\n",
        "1& 1 & 1 & .&.&.&1\\\\\n",
        "X_1^{(1)}&X_2^{(1)} & X_3^{(1)} & .&.&.&X_m^{(1)}\\\\\n",
        ".&. & . &.&.&.& .\\\\\n",
        ".&. & . & .&.&.&.\\\\\n",
        ".&. & . & .&.&.&.\\\\\n",
        "X_1^{(n)}&X_2^{(n)} & X_3^{(n)} & .&.&.&X_m^{(n)}\\\\\n",
        "\\end{bmatrix}_{(n+1) \\times m}\n",
        "\\end{equation}\n",
        "\n",
        "Se puede ver que la suma interior al cuadrado se puede expresar de la siguiente forma:\n",
        "\n",
        "$$\\theta_0 + \\theta_1 X^{(1)}_i + \\cdots + \\theta_n X^{(n)}_i - Y_i = \\Theta^{T}X_i - Y_i^T$$\n",
        "\n",
        "Donde $X_i$ es la i-esima columna de la matriz $X$ y $\\Theta^T , Y^T$ son las transpuestas de los vectores (o matrices columna) $\\Theta, Y$. Es fácil ver que dimensionalmente el producto y resta entre matrices está correcto, pues al transponer los vectores, su dimensión como matriz se invierte, por lo que el producto entre $\\Theta^T$ y $X$ es posible y arroja una matriz $(1\\times m)$, que es la misma dimensión de $Y^T$. Como la resta también será una matriz (o vector) $(1 \\times m)$, la suma de los cuadrados de las componentes de dicha diferencia se puede ver como el producto interno del vector consigo mismo, que matricialmente se obtiene multiplicando a la derecha por la transpuesta del vector. Esto implica lo siguiente:\n",
        "\n",
        "$$J(\\theta) = \\frac{1}{2m}(\\Theta^T X - Y^T)(\\Theta^T X - Y^T)^T$$\n",
        "\n",
        "Así, es posible definir la función costo matricialmente. Si introducimos en el último paréntesis la transposición y realizamos el producto, se obtiene la siguiente expresión:\n",
        "\n",
        "$$J(\\theta) = \\frac{1}{2m}(\\Theta^T X (\\Theta^T X)^T - \\Theta^T X Y - Y^T (\\Theta^T X)^T + Y^T Y )$$\n",
        "\n",
        "Como $\\Theta^T X$ y $Y$ son vectores o matrices $(1\\times m)$ y $(m \\times 1)$ respectivamente, se puede emplear la propiedad para los productos punto de dos vectores $a^T b = b^T a$, es decir, se tiene que $\\Theta^TX Y = Y^T (\\Theta^T X)^T$. Además, usando que $(AB)^T = B^T A^T$, la expresión se puede simplificar de la siguiente forma:\n",
        "\n",
        "$$J(\\theta) = \\frac{1}{2m}(\\Theta^T X X^T \\Theta - 2Y^T X^T \\Theta + Y^T Y )$$\n",
        "\n",
        "Por medio de esta función se desea optimizar los valores de $\\theta$ para llegar a un 'buen' modelo $h(\\theta)$ que sea capaz de predecir valores cercanos a los del target $\\hat{Y}$. Como dicha función representa un error cuadrático medio, lo ideal sería minimizarla con respecto a los parámetros $\\theta$. Para ello podemos emplear los resultados sobre mínimos y máximos en el cálculo vectorial, y por tanto necesitamos primero calcular el gradiente $\\nabla_\\theta J$ con respecto a los parámetros $\\theta$. Empleando las propiedades siguientes de los gradientes vectoriales $\\nabla _x b^T x = b$ y $\\nabla _x  x^T A x = 2Ax$, podemos ver que:\n",
        "\n",
        "$$\\nabla_\\theta (\\Theta^T (X X^T) \\Theta) = 2XX^T\\Theta$$\n",
        "\n",
        "$$\\nabla_\\theta ((Y^T X^T) \\Theta) = XY$$\n",
        "\n",
        "Que dimensionalmente están bien, pues $X \\in \\mathbb{R}^{(n+1)\\times m}$, $X^T \\in \\mathbb{R}^{m \\times (n+1)}$, $\\Theta \\in \\mathbb{R}^{(n+1) \\times 1}$, $Y \\in \\mathbb{R}^{m \\times 1}$. Los gradientes anteriores son de dimensión $((n+1)\\times 1)$, lo que significa que son vectores (que es lo esperable del gradiente de una función escalar). Empleando los resultados anteriores, se tiene que (recordando que $Y$ no depende de $\\theta$):\n",
        "\n",
        "$$\\nabla_\\theta J(\\theta) = \\frac{1}{m} ( X X^T \\Theta - XY)$$\n",
        "\n",
        "Con este gradiente obtenido, podemos aplicarlo al propósito de minimizar (o como mínimo extremizar) la función de coste $J$. Anulando el gradiente llegamos a la siguiente ecuación:\n",
        "\n",
        "$$X X^T \\Theta - XY = \\tilde{0}_{(n+1)\\times 1}$$\n",
        "\n",
        "Cuya solución para $\\Theta$ está dada por la siguiente expresión, suponiendo que $XX^T$ es invertible:\n",
        "\n",
        "$$\\Theta = (XX^T)^{-1}XY$$\n"
      ],
      "metadata": {
        "id": "1zDqaKFP_DCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Para los datos del laboratorio anterior aplicar la ecuacion normal.**"
      ],
      "metadata": {
        "id": "OYzaC4a1Alti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hay que recordar que la función es de la forma $y = 2.1 x_1 - 3.1 x_2$, lo que significa que de manera exacta, el mejor modelo lineal para los datos $y$ es $\\theta_0 = 0, \\theta_1 = 2.1, \\theta_2 = -3.1$."
      ],
      "metadata": {
        "id": "V9W-OGCBAtM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defino la ecuacion\n",
        "y = lambda x1,x2: 2.1*x1 - 3.1*x2\n",
        "\n",
        "#hago un sample aleatorio\n",
        "\n",
        "np.random.seed(42)\n",
        "#fijo la semilla para reproducibilidad de resultados\n",
        "\n",
        "m = 1000\n",
        "#numero de datos\n",
        "\n",
        "x1 = (np.random.random(m) - 0.5)*2\n",
        "x2 = (np.random.random(m) - 0.5)*2\n",
        "#numero aleatorio de m puntos en R2\n",
        "\n",
        "Y = y(x1,x2)\n",
        "#funcion evaluada en los m datos\n",
        "\n",
        "X = np.matrix([np.ones(len(x1)),x1,x2])\n",
        "\n",
        "Y = np.matrix(Y).T\n",
        "\n",
        "theta = np.linalg.inv(X @ X.T)@X@Y\n",
        "print('Los valores de Theta obtenidos por medio de la ecuacion normal son: ', theta)"
      ],
      "metadata": {
        "id": "2Ih61xKCAnVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Tomar el dataset de las casas de Boston y construir un modelo de regresión mutivariada.**"
      ],
      "metadata": {
        "id": "7GO743_ZBCsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero vamos a leer los datos del link de la página web y organizar un dataframe con todas las columnas importantes."
      ],
      "metadata": {
        "id": "QkuoX0y1BSNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tomar los datos de las casas de boston y hacer una regresion lineal tomando\n",
        "# el average number of rooms per dwelling.\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
        "\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :3]])\n",
        "\n",
        "target = raw_df.values[1::2, 2]\n",
        "#el target es el Median value of owner-occupied homes in $1000's\n",
        "\n",
        "#df = pd.DataFrame({\"mean_\":target, \"rm\":data[:,5]})\n",
        "\n",
        "df = pd.DataFrame(columns=columns)\n",
        "for i in columns:\n",
        "  df[i] = data[:,columns.index(i)]\n",
        "\n",
        "display(df)"
      ],
      "metadata": {
        "id": "lEqj1algBGJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, el target o valor a predecir va a ser el de la columna MEDV, o *Median value of owner-occupied homes in $1000's*. Vamos a observar primero como son las correlaciones entre las otras columnas con respecto a esta misma."
      ],
      "metadata": {
        "id": "UuMzl7wYBawQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e88f2b5"
      },
      "source": [
        "#este grafico se hizo con ayuda de la IA gemini, documentado esta la explicacion y razonamiento de su codigo\n",
        "\n",
        "#lista de columnas sin el target\n",
        "columns_to_plot = df.columns.drop('MEDV')\n",
        "\n",
        "#numero columnas y filas en la grafica\n",
        "num_cols = 3\n",
        "num_rows = (len(columns_to_plot) + num_cols - 1) // num_cols\n",
        "#aqui selecciono el numero de filas necesarias para graficar la cantidad de columnas que hay, en filas de 3 plots, sin que se pase de 3\n",
        "\n",
        "\n",
        "#se crean num_cols * num_rows subplots\n",
        "fig = make_subplots(rows=num_rows, cols=num_cols,\n",
        "                    subplot_titles=[f'{col} vs MEDV' for col in columns_to_plot])\n",
        "\n",
        "#se grafica el scatterplot en cada uno de los subplots para cada columna vs MEDV\n",
        "for i, col in enumerate(columns_to_plot):\n",
        "    row = i // num_cols + 1\n",
        "    #se ubica en la fila que corresponde al elemento i-esimo de las columnas del df\n",
        "\n",
        "    col_num = i % num_cols + 1\n",
        "    #se ubica en la columna que corresponde al elemento i-esimo de las columnas del df\n",
        "\n",
        "    fig.add_trace(go.Scatter(x=df[col], y=df['MEDV'], mode='markers', name=col),\n",
        "                  row=row, col=col_num)\n",
        "    #se grafica en el subplot (usando plotly) el scatter de la i-esima columna del df vs MEDV\n",
        "\n",
        "#se fija el tamaño de todas las figuras\n",
        "fig.update_layout(height=num_rows * 200, width = 900, showlegend=False)\n",
        "\n",
        "#se ponen los labels a las figuras\n",
        "for i, col in enumerate(columns_to_plot):\n",
        "    row = i // num_cols + 1\n",
        "    col_num = i % num_cols + 1\n",
        "    fig.update_xaxes(title_text=col, row=row, col=col_num)\n",
        "    fig.update_yaxes(title_text='MEDV', row=row, col=col_num)\n",
        "\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a emplear las columnas RM (*average number of rooms per dwelling*) y LSTAT (*% lower status of the population*), que son las que visualmente parecen comportarse mas linealmente y sin problemas (mucha dispersión o topes en los datos) con respecto a MEDV. Vamos a estandarizar estas variables empleando la siguiente formula\n",
        "\n",
        "$$X_s = \\frac{x - \\mu}{\\sigma}$$\n",
        "\n",
        "Con esto pretendemos escalar de forma semejante las dos variables y el target para evitar problemas de cálculo a la hora de aplicar la ecuación normal. Como se plantea en el ejercicio, propondremos un modelo lineal multivariado para el target MEDV con RM y LSTAT como features, y obtendremos los valores idóneos de $\\theta_0, \\theta_1, \\theta_2$ aplicando la ecuación normal que se dio en la teoría.\n"
      ],
      "metadata": {
        "id": "MtF0FDMGDupR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = len(df)\n",
        "#longitud de los datos\n",
        "\n",
        "ls = (df['LSTAT']-df['LSTAT'].mean())/df['LSTAT'].std()\n",
        "rm = (df['RM']-df['RM'].mean())/df['RM'].std()\n",
        "#se estandarizan los features lstat y rm\n",
        "\n",
        "X = np.matrix([np.ones(m),ls,rm])\n",
        "#se define X como matriz (n+1)xm\n",
        "\n",
        "Y = np.matrix(df['MEDV']).T\n",
        "#se define Y como matriz mx1\n",
        "\n",
        "theta = np.linalg.inv(X @ X.T)@X@Y\n",
        "#se calcula theta usando la ecuacion normal\n",
        "\n",
        "print('Los valores de Theta obtenidos por medio de la ecuacion normal son: ', theta)"
      ],
      "metadata": {
        "id": "Go0mgiqNE5AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez conseguidos los valores de $\\theta$, podemos visualizar (gracias a las pocas dimensiones de los features) que tan bueno es el modelo comparado con los datos del dataset, usando una gráfica de dispersión en 3D."
      ],
      "metadata": {
        "id": "P1iGcVb8Cjgk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd600a47"
      },
      "source": [
        "#nuevamente se uso la IA gemini para hacer el grafico 3D, la siguiente documentacion explica lo que hizo\n",
        "\n",
        "#se extraen los datos sin estandarizar del df\n",
        "lstat_data = df['LSTAT']\n",
        "rm_data = df['RM']\n",
        "medv_data = df['MEDV']\n",
        "\n",
        "#se sacan los maximos y minimos de los features lstat y rm\n",
        "lstat_min, lstat_max = lstat_data.min(), lstat_data.max()\n",
        "rm_min, rm_max = rm_data.min(), rm_data.max()\n",
        "\n",
        "#con ayuda de los minimos anteriores, se hace un grid regular para evaluar el modelo lineal multivariado 2D\n",
        "lstat_grid, rm_grid = np.meshgrid(np.linspace(lstat_min, lstat_max, 100),\n",
        "                                  np.linspace(rm_min, rm_max, 100))\n",
        "\n",
        "\n",
        "#se definen los features estandarizados que se obtienen de la malla regular\n",
        "lstat_grid_scaled = (lstat_grid - df['LSTAT'].mean()) / df['LSTAT'].std()\n",
        "rm_grid_scaled = (rm_grid - df['RM'].mean()) / df['RM'].std()\n",
        "\n",
        "#se hace la prediccion con los theta del modelo lineal\n",
        "medv_plane = theta[0, 0] + theta[1, 0] * lstat_grid_scaled + theta[2, 0] * rm_grid_scaled\n",
        "\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "#se grafican los datos originales como scatter\n",
        "fig.add_trace(go.Scatter3d(x=lstat_data, y=rm_data, z=medv_data, mode='markers', name='Data Points'))\n",
        "\n",
        "#se grafica el plano ajustado por el modelo lineal en 3D\n",
        "fig.add_trace(go.Surface(x=lstat_grid, y=rm_grid, z=medv_plane, name='Fitted Plane', opacity=0.7))\n",
        "\n",
        "#se definen los labels\n",
        "fig.update_layout(\n",
        "    title='Grafica de dispersion 3D y modelo lineal multivariado (MEDV vs LSTAT y RM)',\n",
        "    scene = dict(\n",
        "        xaxis_title='LSTAT',\n",
        "        yaxis_title='RM',\n",
        "        zaxis_title='MEDV'\n",
        "    )\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como se puede observar, aunque los datos son bastante dispersos, el plano parece seguir la tendencia en RM y LSTAT que siguen los datos del target, además de que se ven centrado en un \"valor medio\" de los datos del target."
      ],
      "metadata": {
        "id": "oNq0yiDtFTmD"
      }
    }
  ]
}