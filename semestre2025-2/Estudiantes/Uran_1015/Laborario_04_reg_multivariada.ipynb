{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Salomon Uran Parra C.C. 1015068767**"
      ],
      "metadata": {
        "id": "fah9gSbmsgi0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Laboratorio 4 - Aprendizaje Estadistico**"
      ],
      "metadata": {
        "id": "00sXKTa7sm-E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_0Nax0DscNy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import sklearn as sk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Para simular un conjunto de características $x_1$ , $x_2$,..., $x_n$ trabajaremos en la primera parte con dos características de datos aleatorios que presentan un plano y mostraremos que los párametros optimizados se corresponden con el valor esperado.**\n",
        "\n",
        "- Definir la ecuación  $y = 2.1*x_1 - 3.1*x_2$, y generar números aleatorios que pertenecen al plano.\n",
        "\n",
        "- Realizar un diagrama 3D de los puntos generados aleatoriamente.\n",
        "\n",
        "\n",
        "Nuestro objetivo será encontrar los valores $\\theta_0 = 0, \\theta_1=2.1, \\theta_1=-3.1$ que mejor ajustar el plano, empleando cálculos vectorizados."
      ],
      "metadata": {
        "id": "15CCLtVcsqkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defino la ecuacion\n",
        "y = lambda x1,x2: 2.1*x1 - 3.1*x2\n",
        "\n",
        "#hago un sample aleatorio\n",
        "\n",
        "np.random.seed(42)\n",
        "#fijo la semilla para reproducibilidad de resultados\n",
        "\n",
        "m = 1000\n",
        "#numero de datos\n",
        "\n",
        "x1 = (np.random.random(m) - 0.5)*2\n",
        "x2 = (np.random.random(m) - 0.5)*2\n",
        "#numero aleatorio de m puntos en R2\n",
        "\n",
        "Y = y(x1,x2)\n",
        "#funcion evaluada en los m datos\n",
        "\n",
        "\n",
        "fig = go.Figure(data=[go.Scatter3d(x=x1, y=x2, z=Y, mode='markers',\n",
        "                                   marker=dict(\n",
        "                                       size=5,\n",
        "                                       color=Y,\n",
        "                                       colorscale='viridis',\n",
        "                                       opacity=0.8\n",
        "                                   ))])\n",
        "fig.update_layout(title='Grafico 3D de la funcion',\n",
        "                     scene = dict(\n",
        "                         xaxis_title='x1',\n",
        "                         yaxis_title='x2',\n",
        "                         zaxis_title='y'),\n",
        "                     margin=dict(l=0, r=0, b=0, t=40))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "LvBFkxJPsupp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Inicializar conjunto de parámetros $\\Theta$ de manera aleatoria.**"
      ],
      "metadata": {
        "id": "H6xTstN5szLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#inicializacion de los parametros aleatoriamente\n",
        "\n",
        "n = 3 #numero de features mas 1\n",
        "\n",
        "theta = (np.random.random(n)-0.5)*10\n",
        "#theta aleatorio entre -5 y 5\n",
        "\n",
        "theta = np.matrix(theta)\n",
        "#se convierte en matriz\n",
        "\n",
        "print(theta.shape)\n",
        "print(theta)"
      ],
      "metadata": {
        "id": "_YhwVdvbs1HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Construir la matrix X con dimensiones $(n+1, m)$, m es el numero de datos de entrenamiento y (n) el número de caracteristicas.**"
      ],
      "metadata": {
        "id": "n21y-0hYs3Qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#construccion de la matriz X (n,m) (aqui n es n+1)\n",
        "\n",
        "X = np.matrix([np.ones(len(x1)),x1,x2])\n",
        "\n",
        "X.shape"
      ],
      "metadata": {
        "id": "M7ZqDfkmxd-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Calcular la función de coste(revise cuidosamente las dimensiones de cada matriz):**\n",
        "\n",
        "  - $h = \\Theta^{T} X $\n",
        "  - $\\Lambda= (h -Y) $\n",
        "  - $\\Lambda*= (h -Y)^2 $\n",
        "  - $\\Lambda= [\\Lambda_1,\\Lambda_2, ...,\\Lambda_m]$\n",
        "  - $J = \\frac{1}{2m} \\sum_{i}^m \\Lambda_i $"
      ],
      "metadata": {
        "id": "cI6WgixZxhBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h = theta @ X\n",
        "#primer calculamos h que es la prediccion inicial, recordar que theta es (1,3) y X es (3,1000), por lo que h es (1,1000)\n",
        "\n",
        "Y = np.matrix(Y)\n",
        "Y.shape\n",
        "#Y tambien es (1,1000)\n",
        "\n",
        "lda = h - Y\n",
        "#ahora calculamos la diferencia entre la prediccion y los valores esperados, (1,1000)\n",
        "\n",
        "ldaa = np.square(lda)\n",
        "#ahora calculamos el cuadrado de las diferencias, esto es (1,1000)\n",
        "\n",
        "J = (1/(2*m))*np.sum(ldaa)\n",
        "#ahora calculamos la funcion de coste sumando sobre ldaa\n",
        "\n",
        "print(J)"
      ],
      "metadata": {
        "id": "oas1GG71xkPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Aplicar el gradiente descendente:**\n",
        "  - Encontrar el gradiente.\n",
        "  $$\\nabla J = \\frac{1}{m}\\Lambda X.T$$\n",
        "  \n",
        "  - Actualizar los nuevos parametros:\n",
        "  $$\\Theta_{n+1}=\\Theta_{n}-\\alpha\\nabla J$$"
      ],
      "metadata": {
        "id": "6bzNGSlXxkbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DJ = lda @ X.T *1/m\n",
        "#calculamos el gradiente como el vector de las diferencias entre h y Y, multiplicado por la matriz transpuesta de los features\n",
        "#aqui recordar que lda es (1,1000) y X.T es (1000,3), luego DJ es (1,3)\n",
        "\n",
        "print('Gradiente de J actual: ',DJ)\n",
        "\n",
        "#se actualizan los nuevos parametros dado un alpha = 0.1\n",
        "alpha = 0.1\n",
        "\n",
        "theta = theta - alpha*DJ\n",
        "#algoritmo de gradiente descendente\n",
        "\n",
        "print('Siguiente theta: ',theta)"
      ],
      "metadata": {
        "id": "q9yM351gxquW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Iterar para encontrar los valores $\\Theta$ que se ajustan el plano.**"
      ],
      "metadata": {
        "id": "5hpz3_NjxrGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#para poder iterar vamos a definir funciones utiles primero\n",
        "\n",
        "ndata = 1000\n",
        "nfeat = 2\n",
        "k1 = 10\n",
        "#numero de datos m, numero de features n, amplitud del intervalo de inicializacion de los features X, k1\n",
        "\n",
        "y = lambda x1,x2: 2.1*x1 - 3.1*x2\n",
        "#funcion a fittear, es decir los labels o targets\n",
        "\n",
        "theta = np.matrix(np.random.random(nfeat+1))\n",
        "#inicializo los parametros\n",
        "\n",
        "\n",
        "#la funcion que arroja la prediccion\n",
        "def modelo_lineal(nfeat, ndata, thetax, k = 2, seed = 42):\n",
        "  np.random.seed(seed)\n",
        "  #reproducibilidad\n",
        "\n",
        "  arr = [np.ones(ndata)]\n",
        "  #se inicializa la matriz de datos con la fila de 1's\n",
        "\n",
        "  for i in range(nfeat):\n",
        "    arr.append((np.random.random(ndata)-0.5)*k)\n",
        "    #se toman m datos aleatorios entre -k/2 y k/2 para el i-esimo feature\n",
        "\n",
        "  X = np.matrix(arr)\n",
        "  #se vuelve una matriz que debe ser (n+1,m)\n",
        "\n",
        "  return X\n",
        "\n",
        "X = modelo_lineal(nfeat,ndata,theta,k = k1)\n",
        "#primera prediccion\n",
        "\n",
        "Y = y(X[1],X[2])\n",
        "#se evaluan los datos\n",
        "\n",
        "Y = np.matrix(Y)\n",
        "#matriz (1,m)\n",
        "\n",
        "\n",
        "#se define la funcion coste como antes\n",
        "def coste(theta, X, Y):\n",
        "  h = theta @ X\n",
        "  #primera prediccion entre theta y X, donde h es (1,1000)\n",
        "\n",
        "  m = X.shape[1]\n",
        "  #numero de datos, osea dimension de columnas de X\n",
        "\n",
        "  lda = h - Y\n",
        "  #diferencia entre prediccion y target\n",
        "\n",
        "  ldaa = np.square(lda)\n",
        "  #matriz (1,1000) que son los cuadrados de las componentes de la diferencia\n",
        "\n",
        "  J = (1/(2*m))*np.sum(ldaa)\n",
        "  #se promedia el vector de los cuadrados\n",
        "\n",
        "  DJ = lda @ X.T *1/m\n",
        "  #se obtiene el gradiente de J en la prediccion, que es (1,1000)x(1000,3), osea (1,3)\n",
        "\n",
        "  return J, DJ\n",
        "\n",
        "alpha = 0.1\n",
        "#se define un alfa\n",
        "\n",
        "eps = 1e-7\n",
        "#una tolerancia\n",
        "\n",
        "#como la fucion de coste para el modelo multilineal es un paraboloide en 3 dimensiones de theta, se puede esperar\n",
        "#de manera analoga a la funcion en 2 dimensiones, que su minimo sea cuando se anule la funcion de coste\n",
        "\n",
        "for i in range(300):\n",
        "  #se define un numero de epochs\n",
        "  J, DJ = coste(theta,X,Y)\n",
        "\n",
        "  if J<eps:\n",
        "    break\n",
        "    #condicion de tolerancia\n",
        "\n",
        "  theta = theta - alpha*DJ\n",
        "  #gradiente descendente\n",
        "\n",
        "print(theta)\n"
      ],
      "metadata": {
        "id": "89gKYY7jx1N9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Reescribir su código como una clase.**"
      ],
      "metadata": {
        "id": "bN_yaSOOx2U9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#se define la clase multilinearregresion\n",
        "class MultilinearRegresion():\n",
        "  def __init__(self, X, Y,n):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    Nfeatures = np.shape(X)[0]\n",
        "    m = np.shape(X)[1]\n",
        "    self.theta = np.matrix(np.random.random(Nfeatures).reshape(Nfeatures, 1))\n",
        "  #se definen en la clase los elementos X, Y, Nfeatures, m, y se inicializa theta\n",
        "\n",
        "  def model(self):\n",
        "    self.h = self.theta.T@self.X\n",
        "  #se define la funcion model() que crea la prediccion h que es una matriz (1,m)\n",
        "\n",
        "\n",
        "  def costo(self):\n",
        "    self.J =  np.mean(np.square(self.h - self.Y))\n",
        "  #se define la funcion costo() que crea el valor de J evaluado en la prediccion\n",
        "\n",
        "  def update_params(self, learning_rate):\n",
        "    \"\"\"\n",
        "    Gradiente descendente\n",
        "    \"\"\"\n",
        "    self.grad = (self.h - self.Y)@self.X.T/m#\n",
        "    self.theta = self.theta - learning_rate*self.grad.T#...\n",
        "  #se define la funcion update_params() que calcula el gradiente de J respecto los parametros, que es (1,1000)x(1000,3) = (1,3)\n",
        "  #y que actualiza el valor de theta por medio del gradiente descendente\n",
        "\n",
        "  def fit(self, learning_rate):\n",
        "    for i in range(0, n):\n",
        "      self.model()\n",
        "      self.costo()\n",
        "      self.update_params(learning_rate)\n",
        "    return self.theta\n",
        "  #se define la funcion fit() que itera sobre una cantidad de epochs, n, la prediccion, la funcion de costo, y la actualizacion de parametros, y retorna al final los parametros theta"
      ],
      "metadata": {
        "id": "RXWyW-6yx89J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 500\n",
        "#se define un numero de epochs\n",
        "\n",
        "regresion = MultilinearRegresion(X,Y,n)\n",
        "#se define el modelo de regresion\n",
        "\n",
        "theta = regresion.fit(0.1)\n",
        "#se ejecuta la funcion fit() conn learning_rate = 0.1\n",
        "\n",
        "print('El conjunto de parametros theta0, theta1 y theta2 obtenidos es: ',theta)"
      ],
      "metadata": {
        "id": "mlIXc3ezyA9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Asi, vemos que tanto la clase como la iteracion del punto 6 recuperan aproximadamente (dependiendo del numero de epochs n), el conjunto de parametros que dan el target $y$, los cuales son $\\theta_0 = 0, \\theta_1 = 2.1, \\theta_2 = -3.1$"
      ],
      "metadata": {
        "id": "Meu22KHVI7wx"
      }
    }
  ]
}