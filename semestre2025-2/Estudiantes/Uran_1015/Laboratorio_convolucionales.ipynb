{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Salomon Uran Parra C.C. 1015068767**"
      ],
      "metadata": {
        "id": "v0otC7bR2snk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Laboratorio Ultimo - Aprendizaje Estadistico**\n",
        "\n",
        "Objetivo: Implementar un red neuronal LeNet5 empleando keras e implementar una red neuronal  VGG.  "
      ],
      "metadata": {
        "id": "3S4EpAE_2xQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Importar las librerias:**"
      ],
      "metadata": {
        "id": "bKJb17013H2W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHOKPgud2qJr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Cargar los datos de entrenamiento y test**\n",
        "```python\n",
        "(X_train,y_train),(X_test,y_test)=keras.datasets.mnist.load_data()\n",
        "```"
      ],
      "metadata": {
        "id": "FniO22Ki3Kbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train,y_train),(X_test,y_test)=keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "id": "mRj0O-MJ3M2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Normalizar los datos.**"
      ],
      "metadata": {
        "id": "xidKBYiq3Obv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#se normalizan los datos a un rango entre 0 y 1\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ],
      "metadata": {
        "id": "6LnfwY-M3SOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Realizar una visualización de 20 imagenes aproximadamente, puede emplear el comando imshow con cmap= binary**\n",
        "\n",
        "```python\n",
        "  ax.imshow(X_train[i],cmap='binary')\n",
        "```"
      ],
      "metadata": {
        "id": "U2zoiaIK3Twf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El siguiente codigo permite visualizar una muestra de 20 imagenes del dataset de entrenamiento."
      ],
      "metadata": {
        "id": "uH3BcdmrL1mD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(4,5, figsize=(10,8))\n",
        "fig.tight_layout()\n",
        "for i, axi in enumerate(ax.flatten()):\n",
        "  if i < len(X_train):\n",
        "    axi.imshow(X_train[i], cmap='binary')\n",
        "    axi.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9Ws_x6qd3VtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Implementar en keras, la red Letnet5, la arquitectura de la red es la siguiente:**\n",
        "\n",
        "![img](https://github.com/hernansalinas/Curso_aprendizaje_estadistico/blob/main/Sesiones/convolution_img/LeNet5.png?raw=true)\n"
      ],
      "metadata": {
        "id": "Z-hCnj9R3XGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#primero veamos las dimensiones de una muestra del dataset}\n",
        "X_train[0].shape"
      ],
      "metadata": {
        "id": "Rkl5iN-xNJm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "En la siguiente celda se define una red neuronal convolucional con arquitectura LeNet-5, de la siguiente forma. El input inicial seran muestras 28x28x1 (una imagen). Primero se les aplicam 6 filtros 5x5 (ver imagen) con funcion ReLu, luego un avgpool con pool_size de 2x2 y con un stride (o paso) de 2. Al output de este avgpool, se le aplica nuevamente filtros (esta vez seran 16 5x5 con ReLu) y el mismo avgpool, para luego usar un flatten y entrar el resultado a un fully-connected NN. Esta NN tendra 2 capas ocultas de 120 y 84 neuronas con funcion ReLu y una salida de 10 neuronas con SoftMax."
      ],
      "metadata": {
        "id": "soc3YKUlMkLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(filters=6, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
        "    keras.layers.AveragePooling2D(pool_size=(2, 2), strides=2),\n",
        "    keras.layers.Conv2D(filters=16, kernel_size=(5, 5), activation='relu'),\n",
        "    keras.layers.AveragePooling2D(pool_size=(2, 2), strides=2),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(units=120, activation='relu'),\n",
        "    keras.layers.Dense(units=84, activation='relu'),\n",
        "    keras.layers.Dense(units=10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "GLH7cfh03bVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6. Revisa el modelo que acabaste de construir.**\n",
        "```python\n",
        "model.summary()\n",
        "```"
      ],
      "metadata": {
        "id": "J4Lh9svr3f5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "USu3pz1v3i4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Vamos a utlizar un optimizador de Adams,  El optimizador de Adam (Adaptive Moment Estimation) combina las ventajas de los algoritmos RMSProp y Momentum para mejorar el proceso de aprendizaje de un modelo. Al igual que Momentum, Adam utiliza una estimación del momento y de la magnitud de los gradientes  para actualizar los parámetros del modelo en cada iteración. Sin embargo, en lugar de utilizar una tasa de aprendizaje constante para todos los parámetros, Adam adapta la tasa de aprendizaje de cada parámetro individualmente en función de su estimación del momento y de la magnitud del gradiente. Esto permite que el modelo se ajuste de manera más eficiente y efectiva a los datos de entrenamiento, lo que puede llevar a una mayor precisión de la predicción en comparación con otros métodos de optimización.**"
      ],
      "metadata": {
        "id": "N876aFji3kIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#vamos a usar el optimizador adam junto con una funcion de perdida sparse_categorical_crossentropy, que sirve para clasificar multiples clases dentro de las imagenes\n",
        "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "couCtuEL3rr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8. Realiza el fit del modelo, emplea GPU, para ello cambia la configuración de collaboratory para que tu modelo se ejecute un poco mas rápido.**"
      ],
      "metadata": {
        "id": "lrXHtvH-3-ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train,y_train,epochs=10,validation_split=0.3)"
      ],
      "metadata": {
        "id": "79uW0fcA4FMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **9. Realiza la predicción:**\n",
        "```python\n",
        "q=model.predict(X_test)\n",
        "```"
      ],
      "metadata": {
        "id": "7cit7IoE4bCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q = model.predict(X_test)"
      ],
      "metadata": {
        "id": "qyLgGej84eEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **10. Muestra los valores de q y determina que numero se esta prediciendo.**"
      ],
      "metadata": {
        "id": "4ksPk1ix4fkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X_test[0], cmap='binary')\n",
        "print(\"Probabilidades predichas para la primera imagen:\")\n",
        "print(q[0])\n",
        "predicted_number = np.argmax(q[0])\n",
        "print(f\"El numero predicho corresponde al indice de la mayor probabilidad: {predicted_number}\")"
      ],
      "metadata": {
        "id": "AWr5V6GT4iRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **11. Puede graficar la convergencia del modelo con los siguiente código**\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "# Graficar la curva de loss\n",
        "plt.plot (history.history ['loss'], label='loss')\n",
        "plt.plot (history.history ['val_loss'], label='val_loss')\n",
        "plt.title ('Curva de loss')\n",
        "plt.xlabel ('Época')\n",
        "plt.ylabel ('Loss')\n",
        "plt.legend ()\n",
        "plt.show ()\n",
        "# Graficar la curva de accuracy\n",
        "plt.plot (history.history ['accuracy'], label='accuracy')\n",
        "plt.plot (history.history ['val_accuracy'], label='val_accuracy')\n",
        "plt.title ('Curva de accuracy')\n",
        "plt.xlabel ('Época')\n",
        "plt.ylabel ('Accuracy')\n",
        "plt.legend ()\n",
        "plt.show ()\n",
        "```"
      ],
      "metadata": {
        "id": "D3pUoTyM4j6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot (history.history ['loss'], label='loss')\n",
        "plt.plot (history.history ['val_loss'], label='val_loss')\n",
        "plt.title ('Curva de loss')\n",
        "plt.xlabel ('Época')\n",
        "plt.ylabel ('Loss')\n",
        "plt.legend ()\n",
        "plt.show ()\n",
        "\n",
        "\n",
        "plt.plot (history.history ['accuracy'], label='accuracy')\n",
        "plt.plot (history.history ['val_accuracy'], label='val_accuracy')\n",
        "plt.title ('Curva de accuracy')\n",
        "plt.xlabel ('Época')\n",
        "plt.ylabel ('Accuracy')\n",
        "plt.legend ()\n",
        "plt.show ()"
      ],
      "metadata": {
        "id": "CiXu978M4tp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver que el modelo se desempeña muy bien tanto en el set de entrenamiento como de test, lo que significa que ademas de aprender bien los datos de entrenamiento, fue capaz de generalizarlo bien a datos por fuera de estos."
      ],
      "metadata": {
        "id": "bVGRzztV6Z5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **12. Una forma alterna de implementar el modelo puede ser dada de la siguiente forma:**\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "X_train = X_train.reshape(60000,28,28,1)\n",
        "X_test = X_test.reshape(10000,28,28,1)\n",
        "input_shape = (28,28,1)\n",
        "model = Sequential()\n",
        "model.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "#model.add(Dropout(0.25))\n",
        "model.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "#model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(84, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "l5LDXVUq4xF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **13. Emplea la arquitectura anterior para el dataset cifar100,  empleando BatchNormalization y dropout.**"
      ],
      "metadata": {
        "id": "tfevYc4j42kT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Empleando la arquitectura mostrada en el punto 12, junto con bathnormalization y dropouts (pequeños), se va a entrar la red usando el dataset cifar100, el cual contiene imagenes de las siguientes superclases:\n",
        "\n",
        "aquatic mammals\n",
        "fish\n",
        "flowers\n",
        "food containers\n",
        "fruit and vegetables  \n",
        "household electrical devices  \n",
        "household furniture\n",
        "insects         \n",
        "large carnivores         \n",
        "large man-made outdoor things   \n",
        "large natural outdoor scenes     \n",
        "large omnivores and herbivores      \n",
        "medium-sized mammals         \n",
        "non-insect invertebrates        \n",
        "people         \n",
        "reptiles       \n",
        "small mammals         \n",
        "trees           \n",
        "vehicles 1          \n",
        "vehicles 2\n",
        "\n",
        "Dentro de estas 20 superclases hay un total de 100 clases distintas de objetos (animales, personas, etc) y la idea es que la red logre clasificar correctamente los elementos sin presentar overfitting. Para ello se usala el batchnormalization y el dropout con el siguiente codigo analogo a los anteriormente usados para el dataset de los digitos."
      ],
      "metadata": {
        "id": "C6hZju8d7SfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_image, train_label) , (test_image, test_label) = keras.datasets.cifar100.load_data()\n",
        "\n",
        "train_image = train_image / 255.0\n",
        "test_image = test_image / 255.0\n",
        "\n",
        "plt.imshow(train_image[0])\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "input_shape = (32, 32, 3) #las imagenes de cifar100 son 32x32 en RGB\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\n",
        "model.add(BatchNormalization()) #normaliza los outputs de las funciones de activacion\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "model.add(Dropout(0.05)) #reduce overfitting\n",
        "model.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
        "model.add(Dropout(0.05))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.05))\n",
        "\n",
        "model.add(Dense(84, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.05))\n",
        "\n",
        "model.add(Dense(100, activation='softmax')) #salida de las 100 clases de cifar100"
      ],
      "metadata": {
        "id": "QGLbR-_t47jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **14. Emplea el siguiente compilador:**\n",
        "\n",
        "El optimizador de NAdam (Nesterov-accelerated Adaptive Moment Estimation) es una variante de Adam que incorpora el método de Nesterov, que consiste en utilizar una predicción de la posición futura de los parámetros para calcular el gradiente, en lugar de la posición actual. Esto hace que el algoritmo sea más sensible a los cambios de dirección del gradiente y evite oscilaciones innecesarias. NAdam también modifica la forma de calcular el momento y la magnitud del gradiente, usando una media móvil exponencial sesgada hacia cero en lugar de una media móvil exponencial simple.\n",
        "\n",
        "\n",
        "```python\n",
        "model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "El número de épocas que se necesita para entrenar una red neuronal depende de varios factores, como el tamaño de los datos, la complejidad del modelo, la función de pérdida, el algoritmo de optimización, la tasa de aprendizaje, etc. No hay una regla fija para elegir el número de épocas, pero se puede usar el criterio de parada temprana, que consiste en monitorear el error de validación y detener el entrenamiento cuando este empiece a aumentar, lo que indica un sobreajuste del modelo.\n",
        "\n"
      ],
      "metadata": {
        "id": "vBMZqeSS5gvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **15. Emplea early_stooping y realiza el fit**\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
        "```\n",
        "\n",
        "El parámetro patience=5 indica el número de épocas sin mejora después de las cuales se detendrá el entrenamiento. El parámetro restore_best_weights=True indica que se restaurarán los pesos del modelo desde la época con el mejor valor de la métrica monitoreada. Esto puede ayudar a evitar el sobreajuste y mejorar el rendimiento del modelo\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "history = model.fit(train_image, train_label, epochs=30, validation_split=0.2 , batch_size=64, callbacks=[early_stopping])\n",
        "```"
      ],
      "metadata": {
        "id": "zf8kccCF5l-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Empleando el compilador del punto 14 y el early_stopping, se entrenara la arquitectura del punto 13 en el dataset cifar100. Nuevamente se usa una funcion sparse_categorical_crossentropy como perdida, se entrenara un total de 30 epocas o hasta que el early_stopping detenga el entrenamiento."
      ],
      "metadata": {
        "id": "GFxq3euw-LsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "early_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "history = model.fit(train_image, train_label, epochs=30, validation_split=0.2 , batch_size=64, callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "O-1V5VQC5omS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos ver que a la red le cuesta no solo aprender los datos de entrenamiento (pues son demasiadas clases en una red no tan compleja) sino que le cuesta mucho mas generalizar lo aprendido. Los porcentajes de precision tanto en el entrenamiento como test deberian poder mejorarse explorando nuevas topologias de la red junto con el cambio de los porcentajes en el dropout y la ubicacion de los batchnormalization dentro de la red."
      ],
      "metadata": {
        "id": "JxqCAqLyA8UC"
      }
    }
  ]
}