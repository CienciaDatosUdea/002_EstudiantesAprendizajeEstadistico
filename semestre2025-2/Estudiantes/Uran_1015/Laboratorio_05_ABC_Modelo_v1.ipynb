{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Salomon Uran Parra C.C. 1015068767**"
      ],
      "metadata": {
        "id": "YNCtcGB7SD0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Laboratorio 5 ABC - Aprendizaje Estadistico**"
      ],
      "metadata": {
        "id": "a_y_MIDeSU1b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvESOk8qSCae"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Leer el data frame en formato csv en la dirección https://raw.githubusercontent.com/hernansalinas/Curso_aprendizaje_estadistico/main/datasets/Sesion_07_housing.csv**"
      ],
      "metadata": {
        "id": "FoBRhAN4SXdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/hernansalinas/Curso_aprendizaje_estadistico/main/datasets/Sesion_07_housing.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "JXFr5dZNSdJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Entender  el estado de los datos, para ello puedo emplear los comandos básicos del pandas**\n",
        "\n",
        "  ```python\n",
        "  df.info()\n",
        "  df.describe()\n",
        "  df.isnull().sum()\n",
        "  df.isna().sum()\n",
        "```\n"
      ],
      "metadata": {
        "id": "PuzXC4q8SfzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "On-TX3UXSiDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hay una gran cantidad de datos NaN en la columna de total_bedrooms, por lo que se deberá plantear una estrategia para su limpieza o eliminación."
      ],
      "metadata": {
        "id": "QXTw_uZBVyf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#por ejemplo podemos eliminar los datos NaN\n",
        "df.dropna(subset=[\"total_bedrooms\"], inplace=True)\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df.info()"
      ],
      "metadata": {
        "id": "1pW6iCnO4gzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Determinar los elementos únicos dentro de la columna ocean_proximity.**"
      ],
      "metadata": {
        "id": "3-oM0aeDSjrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['ocean_proximity'].unique())\n",
        "print(df.ocean_proximity.unique())"
      ],
      "metadata": {
        "id": "nd_jJxT1SqkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Para las columnas**\n",
        "\n",
        "\n",
        "```python\n",
        "cols = [\"housing_median_age\",\t\"total_rooms\",\t\"total_bedrooms\",\t\"population\",\t\"households\",\t\"median_income\",\t\"median_house_value\"]\n",
        "```\n",
        "\n",
        "Determinar el promedio de cada una de las columnas asociado a cada elementos unico de ocean_proximity, intenta con la operación groupby."
      ],
      "metadata": {
        "id": "EquBd8hsSrij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [\"housing_median_age\",   \"total_rooms\",  \"total_bedrooms\",   \"population\",   \"households\",   \"median_income\",    \"median_house_value\",'longitude','latitude']\n",
        "df.groupby('ocean_proximity')[cols].mean()"
      ],
      "metadata": {
        "id": "n6pS9HQySuRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Construye un histograma para cada columna, puede emplear la libreria de seaborn.**"
      ],
      "metadata": {
        "id": "EvLEntgeSvgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "\n",
        "#empleando las columnas del punto anterior\n",
        "for i in cols:\n",
        "  plt.subplot(3,4,cols.index(i)+1)\n",
        "  sns.histplot(df, x = i, kde = True)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BVN0zcHnSxSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7. Empleando el siguiente código realiza el gráfico boxplot,**\n",
        "```python\n",
        "#draw boxplot\n",
        "df.boxplot(column=\"median_house_value\", by='ocean_proximity', sym = 'k.', figsize=(18,6))\n",
        "#set title\n",
        "plt.title('Boxplot for comparing price per living space for each city')\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "Ft4h0ha7SyZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#grafica el diagrama de caja del median_house_value para cada uno de los datos de ocean_proximity\n",
        "df.boxplot(column=\"median_house_value\", by='ocean_proximity', sym = 'k.', figsize=(18,6))\n",
        "\n",
        "plt.title('Diagrama de caja de cercania al oceano versus valor medio de un hogar')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_D_JQa9BTKPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8. Determina la matrix de correlación.**\n",
        "\n",
        "```python\n",
        "corr_matrix = df.corr()\n",
        "corr_matrix\n",
        "\n",
        "plt.figure(figsize = (10,6))\n",
        "sns.heatmap(corr_matrix, annot = True, cmap = \"coolwarm\", center=0)\n",
        "plt.show()\n",
        "```"
      ],
      "metadata": {
        "id": "mvazIbziTXJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#primero crearemos un dataframe que contenga solo las columnas numericas del df\n",
        "ndf = df.select_dtypes(include=[np.number])\n",
        "\n",
        "corr_matrix = ndf.corr()\n",
        "#se obtiene la matriz de correlacion\n",
        "\n",
        "plt.figure(figsize = (10,6))\n",
        "sns.heatmap(corr_matrix, annot = True, cmap = \"coolwarm\", center=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GD9vzhGRTbjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **9. con las columnas, realiza un grafico pairplot empleando seaborn  de python.**\n",
        "```python\n",
        "cols = [\"median_house_value\", \"median_income\", \"total_rooms\",\"housing_median_age\"]\n",
        "```"
      ],
      "metadata": {
        "id": "q4Wmym1FTcmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [\"median_house_value\", \"median_income\", \"total_rooms\",\"housing_median_age\"]\n",
        "sns.pairplot(df[cols])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ocFTxIIvTfh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **10. Realiza un scatter plot con la libreria sea born de python, el color del grafico puede ser empleado con la columna median_house_value**"
      ],
      "metadata": {
        "id": "lvtG0C7bTfv0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6f848d0"
      },
      "source": [
        "# Assuming 'df' is your pandas DataFrame\n",
        "sns.scatterplot(data=df, x=\"median_income\", y=\"housing_median_age\", hue=\"median_house_value\", palette=\"viridis\")\n",
        "plt.title('Scatter plot of Median Income vs Median House Value (colored by Median House Value)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **11. ¿Las siguiente linea es adecuada para separar el dataframe en datos de entrenamiento de test?, ¿que pasa en la división de los datos?**\n",
        "\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ¿Es significativa la muestra que se esta considerando?\n",
        "train_set, test_set \\\n",
        "  = train_test_split(df, test_size = 0.2, random_state = 42)\n",
        "\n",
        "print(len(train_set))\n",
        "print(len(test_set))\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "E0QFNm80Tkm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set \\\n",
        "  = train_test_split(df, test_size = 0.2, random_state = 42)\n",
        "\n",
        "print(len(train_set))\n",
        "print(len(test_set))"
      ],
      "metadata": {
        "id": "s85lrb_BTmlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El anterior código está haciendo una separación aleatoria de los datos del df, y dedicando un 80% a entrenamiento del modelo y 20% a testeo de este. Esta estrategia es simple pero buena para generar remuestreos de los datos solo si los datos en el dataframe están distribuidos uniformemente y puedo asegurar que los nuevos muestreos conservan las proporciones y características del primero. Justamente esto último me permite asegurar que los conjuntos son representativos tanto para un entrenamiento del modelo como para un test del mismo."
      ],
      "metadata": {
        "id": "Eh4QIeVb94zs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **12. División del dataset en grupos:**"
      ],
      "metadata": {
        "id": "-aBQI_eQTtlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La siguiente celda define un nuevo feature, income_cat, que divide en 5 categorias distintas los ingresos medios del dataframe."
      ],
      "metadata": {
        "id": "BHyvgUHUmz8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"income_cat\"] = pd.cut(df[\"median_income\"],\n",
        "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
        "                               labels=[1, 2, 3, 4, 5])\n",
        "\n",
        "display(df.head())\n",
        "df.income_cat.hist()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "US7CmZfaTudE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, debido a que las categorías no tienen las mismas proporciones entre si (hay más o menos cantidad de una u otra), el remuestreo se hará basado en un shuffle (permutación) estratificado, de la siguiente forma:"
      ],
      "metadata": {
        "id": "X60kCFhDnKXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split = StratifiedShuffleSplit(n_splits = 1, test_size=0.2, random_state=42)\n",
        "\n",
        "for train_index, test_index in split.split(df, df[\"income_cat\"]):\n",
        "  strat_train_set = df.loc[train_index]\n",
        "  strat_test_set = df.loc[test_index]\n",
        "\n",
        "strat_train_set.reset_index(drop=True, inplace=True)\n",
        "strat_test_set.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "WcYULDsjVRjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El código anterior hace los remuestreos del dataframe para obtener dos conjuntos de datos, un strat_train_set con 80% de los datos del df y un strat_test_set con el 20% restante. Teniendo en cuenta las proporciones (histograma) dadas por las categorías de ingresos medios que se definieron arriba, se debe cumplir que ambos conjuntos de datos nuevos poseen las mismas proporciones para cada categoría. También se comparará con el método anterior de sampleo aleatorio para el remuestreo."
      ],
      "metadata": {
        "id": "1xywuVbJn9S4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"income_cat\"].value_counts() / len(df))\n",
        "\n",
        "print(strat_train_set[\"income_cat\"].value_counts() / len(strat_train_set))\n",
        "\n",
        "print(strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set))\n",
        "\n",
        "train_set, test_set \\\n",
        "  = train_test_split(df, test_size = 0.2, random_state = 42)\n",
        "\n",
        "print(train_set[\"income_cat\"].value_counts() / len(train_set))\n",
        "print(test_set[\"income_cat\"].value_counts() / len(test_set))"
      ],
      "metadata": {
        "id": "bSMlmI5zVK_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Del anterior print podemos ver que efectivamente, el shuffle estratificado funciona mucho mejor para preservar las proporciones del dataframe original en los nuevos conjuntos de entrenamiento y test, lo que los vuelve más representativos e ideales para dichos propósitos. El sampleo aleatorio en cambio, se aleja un poco de las proporciones del dataframe, como se puede observar en los últimos dos prints.\n",
        "\n",
        "Por último, el siguiente código agrupa de forma clara los remuestreos aleatorios y estratificados para el conjunto de testeo, y crea dos métricas de errores porcentuales de los mismos respecto al dataframe original:\n"
      ],
      "metadata": {
        "id": "GPoEYkwWpfdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def income_cat_proportions(data):\n",
        "    return data[\"income_cat\"].value_counts() / len(data)\n",
        "\n",
        "train_set, test_set = train_test_split(df, test_size = 0.2, random_state = 42)\n",
        "\n",
        "compare_props = pd.DataFrame({\n",
        "    \"Overall\": income_cat_proportions(df),\n",
        "    \"Stratified\": income_cat_proportions(strat_test_set),\n",
        "    \"Random\": income_cat_proportions(test_set),\n",
        "}).sort_index()\n",
        "\n",
        "compare_props[\"Rand. %error\"] =abs( 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100)\n",
        "compare_props[\"Strat. %error\"] =abs( 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100)\n",
        "\n",
        "compare_props"
      ],
      "metadata": {
        "id": "fdVbuYXrVhY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí podemos ver claramente la gran ventaja de usar el remuestreo estratificado, pues indica que los subconjuntos derivados del mismo son representativos y muy similares al original, a diferencia del remuestreo aleatorio."
      ],
      "metadata": {
        "id": "2xLBL1d4qwYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **13. Puedes agregar nuevas variables al dataframe para el análisis, por ejemplo:**\n",
        "```python\n",
        "df_train[\"rooms_per_household\"] = df_train[\"total_rooms\"]/df_train[\"households\"]\n",
        "df_train[\"bedrooms_per_room\"] = df_train[\"total_bedrooms\"]/df_train[\"total_rooms\"]\n",
        "df_train[\"population_per_household\"]=df_train[\"population\"]/df_train[\"households\"]\n",
        "```\n"
      ],
      "metadata": {
        "id": "uxrMiZdmTulL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ahora se definen nuevas variables en el df original\n",
        "df[\"rooms_per_household\"] = df[\"total_rooms\"]/df[\"households\"]\n",
        "df[\"bedrooms_per_room\"] = df[\"total_bedrooms\"]/df[\"total_rooms\"]\n",
        "df[\"population_per_household\"]=df[\"population\"]/df[\"households\"]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "wkfC9YFsTx7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **14. Compara las siguientes variables:**\n",
        "```python\n",
        "imp_mean.statistics_\n",
        "df_train_num.median()\n",
        "```\n",
        "\n",
        "\n",
        "```python\n",
        "Constuye la matriz de características:\n",
        "\n",
        "X = imp_mean.transform(df)\n",
        "housing_tr = pd.DataFrame(X, columns=df_train_num.columns)\n",
        "```"
      ],
      "metadata": {
        "id": "EQ8kc6XITyV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sci-kit learn tiene una herramienta llamada imputer que sirve para rellenar o reemplazar los valores NaN o no validos del dataframe usando diferentes estrategias. A continuación vamos a usarlo para reemplazar los valores de total_bedrooms que son NaN."
      ],
      "metadata": {
        "id": "z9YK2THC2nPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imp_mean = SimpleImputer(strategy=\"mean\")\n",
        "#se define el imputer usando sklearn.SimpleImputer y la estrategia mean, que es tomar la media de la columna como el valor a sustituir en el NaN\n",
        "\n",
        "df1 = df.drop(\"ocean_proximity\", axis=1)\n",
        "#tomamos el dataframe sin la columna de la proximidad al oceano\n",
        "\n",
        "imp_mean.fit(df1)\n",
        "#hacemos el ajuste de las medias para el imputer\n",
        "\n",
        "X = imp_mean.transform(df1)\n",
        "#se transforma el dataframe reemplazando los datos NaN por las medias de sus columnas\n",
        "\n",
        "df2 = pd.DataFrame(X, columns=df1.columns)\n",
        "#redefinimos el dataframe como su transformacion por el imputer\n",
        "\n",
        "#ahora procedamos a unirle la columna ocean proximity\n",
        "df2 = df2.join(df[\"ocean_proximity\"])\n",
        "print(df2.info())\n"
      ],
      "metadata": {
        "id": "nUKXtgx0Vr09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **15.  ¿Qué realizan las siguientes lineas de código?**\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "df_train[\"ocean_proximity\"].unique()\n",
        "housing_cat=df_train[[\"ocean_proximity\"]]\n",
        "housing_cat\n",
        "\n",
        "cat_encoder = OneHotEncoder(sparse_output=False)\n",
        "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
        "print(housing_cat_1hot)\n",
        "print(cat_encoder.categories_)\n",
        "\n",
        "\n",
        "df_cat_1hot = pd.DataFrame(housing_cat_1hot, columns = cat_encoder.categories_[0])\n",
        "\n",
        "housing_tr_ = housing_tr.join(df_cat_1hot)\n",
        "```\n"
      ],
      "metadata": {
        "id": "WNcswpO9T3vz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como la columna ocean_proximity tiene datos no numéricos, no es ideal para entrenar un modelo de regresión con alguna columna como target. Por ello, se emplea la estrategia OneHotEncoder, que permite volver numéricas estas variables no numéricas o tipo string."
      ],
      "metadata": {
        "id": "rlyYZRoU5Z-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strat_train_set[\"ocean_proximity\"].unique()\n",
        "housing_cat=strat_train_set[[\"ocean_proximity\"]]\n",
        "\n",
        "cat_encoder = OneHotEncoder(sparse_output=False)\n",
        "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
        "\n",
        "display(housing_cat_1hot)\n",
        "print(len(housing_cat_1hot))\n",
        "print(cat_encoder.categories_)\n",
        "\n",
        "\n",
        "df_cat_1hot = pd.DataFrame(housing_cat_1hot, columns = cat_encoder.categories_[0])\n",
        "\n",
        "strat_train_set = strat_train_set.join(df_cat_1hot)\n",
        "\n",
        "#vamos a eliminar la columna ocean_proximity\n",
        "strat_train_set = strat_train_set.drop(\"ocean_proximity\", axis=1)\n",
        "\n",
        "strat_train_set"
      ],
      "metadata": {
        "id": "_vm3XJ_UT7u6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **16. Las variables pueden ser escaladas como sigue:**\n",
        "\n",
        "```python\n",
        "\n",
        "cols=[\"longitude\", \"latitude\",\t\"housing_median_age\",\t\"total_rooms\",\\\n",
        "      \"total_bedrooms\",\t\"population\",\t\"households\",\t\"median_income\",\\\n",
        "      \"<1H OCEAN\",\t\"INLAND\",\t\"ISLAND\",\t\"NEAR BAY\", \"NEAR OCEAN\"]\n",
        "\n",
        "\n",
        "housing_scale=housing_tr_[cols]\n",
        "housing_scale\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(housing_scale)\n",
        "\n",
        "X = scaler.transform(housing_scale)\n",
        "\n",
        "\n",
        "housing_prepared = pd.DataFrame(X, columns = housing_scale.columns)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "Z6-PaMm7T8Dz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debido a que las diferentes variables asociadas a cada feature del dataframe poseen escalas y significados diferentes, esto puede representar un serio problema para la convergencia de algún método de optimización del modelo de regresión, por lo que es idóneo escalar las variables a intervalos que sean comparables entre sí. El siguiente código hace lo propio para las columnas indicadas, empleando el método MinMaxScaler que lo que hace es normalizar las columnas respecto a la diferencia entre su máximo y mínimo e inicializarlo en el dato mínimo, de tal forma que los datos van entre 0 y 1. Internamente se aplica la siguiente formula:\n",
        "\n",
        "$$X_S = \\frac{x-x_{\\rm min}}{x_{\\rm max} - x_{\\rm min}}$$\n"
      ],
      "metadata": {
        "id": "xmJibrH4-0pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols=[\"longitude\", \"latitude\",  \"housing_median_age\",   \"total_rooms\",\\\n",
        "      \"total_bedrooms\", \"population\",   \"households\",   \"median_income\",\\\n",
        "      \"<1H OCEAN\",  \"INLAND\",   \"ISLAND\",   \"NEAR BAY\", \"NEAR OCEAN\"]\n",
        "\n",
        "\n",
        "housing_scale = strat_train_set[cols]\n",
        "\n",
        "housing_scale\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(housing_scale)\n",
        "\n",
        "X = scaler.transform(housing_scale)\n",
        "\n",
        "\n",
        "housing_prepared = pd.DataFrame(X, columns = housing_scale.columns)\n",
        "\n",
        "housing_prepared"
      ],
      "metadata": {
        "id": "JNSMxGg8T_kL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **17.** Para todos los pasos anteriores, contruye ordenadamente los pasos limpieza, escalamiento de variables, manejo de texto y atributos categóricos para tener el data frame listo para el análisis. Recuerda dividir el data frame en datos de entrenamiento y de test con la correcta estractificación. Genera dos data frame: housing_train, housing_test, cada una, debe tener las caracteristicas y los datos etiquetados."
      ],
      "metadata": {
        "id": "MUgyn9LxT_s8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora que sabemos cómo realizar todo el procesado y preparación de los datos del dataframe, se puede realizar paso a paso la limpieza y preparación anterior de manera ordenada y lógica."
      ],
      "metadata": {
        "id": "MF_zZG7G_QTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importemos nuevamente el dataframe\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/hernansalinas/Curso_aprendizaje_estadistico/main/datasets/Sesion_07_housing.csv\")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "-vFF5qXjUEAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#como ya sabemos que en los total_bedrooms tiene valores NaN, vamos a usar un imputer para reemplazar dichos datos por la media de la columna\n",
        "#podemos reutilizar el codigo de antes pero ahora en vez de definir un nuevo dataframe df2, reemplazamos df por su transformacion\n",
        "\n",
        "imp_mean = SimpleImputer(strategy=\"mean\")\n",
        "df1 = df.drop(\"ocean_proximity\", axis=1)\n",
        "imp_mean.fit(df1)\n",
        "X = imp_mean.transform(df1)\n",
        "df2 = pd.DataFrame(X, columns=df1.columns)\n",
        "df = df2.join(df[\"ocean_proximity\"])\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "m0OdrKPxBUBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ahora, vamos a definir las nuevas variables y las categorias para los median_income\n",
        "df[\"rooms_per_household\"] = df[\"total_rooms\"]/df[\"households\"]\n",
        "df[\"bedrooms_per_room\"] = df[\"total_bedrooms\"]/df[\"total_rooms\"]\n",
        "df[\"population_per_household\"]=df[\"population\"]/df[\"households\"]\n",
        "df[\"income_cat\"] = pd.cut(df[\"median_income\"],\n",
        "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
        "                               labels=[1, 2, 3, 4, 5])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "_5hC_meICJiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ahora podemos lidiar con el feature no numerico ocean_proximity, empleando la estrategia onehotencoder\n",
        "\n",
        "df[\"ocean_proximity\"].unique()\n",
        "dftrans=df[[\"ocean_proximity\"]]\n",
        "\n",
        "cat_encoder = OneHotEncoder(sparse_output=False)\n",
        "dftrans_1hot = cat_encoder.fit_transform(dftrans)\n",
        "\n",
        "df_cat_1hot = pd.DataFrame(dftrans_1hot, columns = cat_encoder.categories_[0])\n",
        "\n",
        "df = df.join(df_cat_1hot)\n",
        "\n",
        "#vamos a eliminar la columna ocean_proximity\n",
        "df = df.drop(\"ocean_proximity\", axis=1)\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "kQ-vQ77cCsOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ahora, podemos escalar los features del dataframe de manera que sean comparables entre si\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(df)\n",
        "X = scaler.transform(df)\n",
        "df = pd.DataFrame(X, columns = df.columns)\n",
        "df[\"income_cat\"] = df[\"income_cat\"]*4 + 1\n",
        "#revertimos el escalamiento para income_cat\n",
        "df.head()"
      ],
      "metadata": {
        "id": "v5zHxKJwB5gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#una vez preparado y listo el dataframe, podemos hacer un muestreo de datos de entrenamiento y test estratificado para entrenar un modelo multilineal\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits = 1, test_size=0.2, random_state=42)\n",
        "\n",
        "for train_index, test_index in split.split(df, df[\"income_cat\"]):\n",
        "  housing_train = df.loc[train_index]\n",
        "  housing_test= df.loc[test_index]\n",
        "\n",
        "housing_train.reset_index(drop=True, inplace=True)\n",
        "housing_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(len(housing_train))\n",
        "print(len(housing_test))\n",
        "\n",
        "print(df[\"income_cat\"].value_counts() / len(df))\n",
        "\n",
        "print(housing_train[\"income_cat\"].value_counts() / len(housing_train))\n",
        "\n",
        "print(housing_test[\"income_cat\"].value_counts() / len(housing_test))\n",
        "\n",
        "#features y labels para el entreno y test\n",
        "\n",
        "housing_train_features = housing_train.drop(\"median_house_value\", axis=1)\n",
        "housing_train_labels = housing_train[\"median_house_value\"].copy()\n",
        "\n",
        "print(len(housing_train_features))\n",
        "\n",
        "print(len(housing_train_labels))\n",
        "\n",
        "housing_test_features = housing_test.drop(\"median_house_value\", axis=1)\n",
        "housing_test_labels = housing_test[\"median_house_value\"].copy()\n"
      ],
      "metadata": {
        "id": "JUOf4Rm7Gcow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Preguntas adicionales del numeral 17**\n",
        "1. ¿que puede concluir respecto al modelo empleado?\n",
        "2. ¿El modelo de regresión lineal es valido para lo construido,\n",
        "3. ¿qué informacion nos da el score?\n",
        "4. ¿Puede ser ajustado a otro modelo?\n",
        "5. ¿Como puede autmatizar todo el proceso empleando pipelines?"
      ],
      "metadata": {
        "id": "IoCxbOhnHP5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#se crea el modelo lineal\n",
        "lin_reg = LinearRegression()\n",
        "\n",
        "#se hace un fiteo lineal con todos los features disponibles en el dataframe\n",
        "lin_reg.fit(housing_train_features, housing_train_labels)\n",
        "\n",
        "#se evalua el score en los features y labels del test\n",
        "print(lin_reg.score(housing_test_features,housing_test_labels))"
      ],
      "metadata": {
        "id": "3Ft77sjzLOHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debido a la compleja forma de los gráficos de dispersión para algunas columnas del dataframe, inicialmente no sería conveniente pensar que, por ejemplo, el valor medio de las casas pueda ser modelado con una dependencia lineal de alguna de las otras columnas. Sin embargo, aún es posible llevar a cabo dicha regresión. La celda anterior hace lo mencionado y calcula el score del modelo con los conjuntos de testeo definidos arriba. Como podemos observar, aun si incluimos todas las columnas o features del dataframe, el modelo lineal se comporta de manera muy pobre, pues un score de 0.65 indica que solo acierta un 65% de las veces. Esto es un poco más que adivinar el resultado, pero sigue siendo un desempeño muy pobre para las predicciones necesarias en el sector inmobiliario.\n",
        "\n",
        "Muy seguramente, el que las gráficas de dispersión no tengan una forma definida significa que la correlación entre los datos tiene una estructura muy compleja, la cual un modelo lineal no es capaz de aproximar adecuadamente. Por ello se puede pensar mejor en redes neuronales u otro tipo de modelos de regresión, que sean mucho más robustos y complejos y que sean capaces de captar las estructuras del dataframe para una mejor predicción del valor medio de las casas.\n",
        "\n",
        "Todo este proceso de limpieza, organización y preparación de los datos de entrenamiento y testeo, incluso junto con la construcción y evaluación del modelo, pueden ser realizados empleando pipelines. Los pipelines son procesos de manipulación y transformación de datos que consisten inicialmente en tres pasos: Origen de los datos, procesamiento o transformación de los datos, y destino o almacenamiento de estos. Son arquitecturas ordenadas que se encargan de realizar de manera coherente y eficiente los procedimientos de limpieza y preparación que hicimos anteriormente en el código y de forma muy separada. En este caso, podrían usarse librerías de pipelines robustas de Sci-Kit learn para haber realizado los procesamientos anteriores del dataframe.\n"
      ],
      "metadata": {
        "id": "l8n0Iki4PpCz"
      }
    }
  ]
}