{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UG0R-DxjnBhn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import scipy as sc\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pylab as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topología de la red.\n",
        "\n",
        "1. Construir un clase  que permita definir una red neuronal con la topología\n",
        "deseada y la función de activación para cada capa, para ello deberá construir una funcion Topology con el número de capas de la red neuronal :\n",
        "\n",
        "Topology = [n_x, n_h1, n_h2, n_h3, ...,n_y]\n",
        "\n",
        "En este caso:\n",
        "- $n^{[0]}=n_x$ seran los valores de entradas de la capa de entrada\n",
        "- $n^{[1]}=n_{h1}$ Primera capa oculta de la red neuronal\n",
        "- $n^{[2]}=n_{h2}$ Segunda capa oculta de la red neuronal\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "- $n^{[l]}=n_{hl}$ Segunda capa oculta de la red neuronal\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "- $n^{[L]}=n_{y}$ Segunda capa oculta de la red neuronal\n",
        "\n",
        "donde\n",
        "\n",
        "- $\\mathrm{n_x}$: valores de entrada\n",
        "- $\\mathrm{n_{h1}}$: hidden layer 1\n",
        "- $\\mathrm{n_{h2}}$: hidden layer 2\n",
        "- $\\mathrm{n_y}$: last layer\n",
        "\n",
        "- $n^{[L]}=n_{y}$ Segunda capa oculta de la red neuronal\n",
        "\n",
        "\n",
        "También definir una lista con las funciones de activaciones para cada capa.\n",
        "\n",
        "\n",
        "activation=[None, relu, relu, relu, ...,sigmoid]\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "a. Cada unas de las capas deberá tener los parámetros de inicialización de manera aleatoria:\n",
        "\n",
        "\n",
        "La matriz de parametros para cada capa debera tener:\n",
        "\n",
        "\n",
        "$\\mathrm{dim(\\vec{b}^{[l]})}=n^{[l]}$\n",
        "\n",
        "$\\mathrm{dim(\\vec{\\Theta}^{[l]})}=n^{[l]}\\times n^{[l-1]}$\n",
        "\n",
        "Lo anteriores parametros deberán estar en el constructor de la clase.\n",
        "\n",
        "\n",
        "b. Construya un metodo llamado output cuya salida serán los valores de Z y A\n",
        "\n",
        "\n",
        "$\\mathrm{dim(\\vec{\\cal{A}}^{[l]})}=n^{[l-1]}\\times m $\n",
        "\n",
        "$\\mathrm{dim(\\vec{\\cal{Z}}^{[l]})}=n^{[l]}\\times m $."
      ],
      "metadata": {
        "id": "MGZXO6nVNusi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iniciamos con la funciones de activación: Tendriamos primero una función que Retorna el máximo entre 0 y el valor de entrada Z (elemento a elemento). Luego con una función sigmoide que sirve para la capa de salida en clasificación binaria, pero la incluimos. Además de la que está involucrada con la clasificación multiclase. Luego vamos con la inicialización y la porpagación\n"
      ],
      "metadata": {
        "id": "sq6cpdrFOAB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def act_funcion(x, activation):\n",
        "    if activation == \"sigmoid\":\n",
        "        f = 1/(1+np.exp(-x))\n",
        "        fp = f*(1-f)\n",
        "        return f, fp\n",
        "    if activation == \"relu\":\n",
        "        f = np.maximum(0, x)\n",
        "        fp = (x > 0).astype(float)\n",
        "        return f, fp\n",
        "    if activation == \"tanh\":\n",
        "        f = np.tanh(x)\n",
        "        fp = 1 - f**2\n",
        "        return f, fp\n",
        "    return x, np.ones_like(x)\n",
        "\n",
        "class layer_nn:\n",
        "    def __init__(self, act_fun, nlayer_presente, nlayer_before):\n",
        "        self.theta = 2*np.random.random((nlayer_presente, nlayer_before)) - 1\n",
        "        self.B = 2*np.random.random((nlayer_presente, 1)) - 1\n",
        "        self.act_fun = act_fun\n",
        "\n",
        "class RedManual:\n",
        "    def __init__(self, topologia, activaciones, semilla=123):\n",
        "        np.random.seed(semilla)\n",
        "        self.topologia = topologia\n",
        "        self.activaciones = activaciones\n",
        "        self.total_capas = len(topologia) - 1\n",
        "        self.capas = []\n",
        "        for i in range(1, len(topologia)):\n",
        "            n_actual = topologia[i]\n",
        "            n_anterior = topologia[i-1]\n",
        "            act = activaciones[i]\n",
        "            capa = layer_nn(act, n_actual, n_anterior)\n",
        "            self.capas.append(capa)\n",
        "\n",
        "    def salida_completa(self, X):\n",
        "        Zs = {}\n",
        "        As = {}\n",
        "        A_actual = X\n",
        "        As[0] = A_actual\n",
        "\n",
        "        indice = 1\n",
        "        for capa in self.capas:\n",
        "            T = capa.theta\n",
        "            B = capa.B\n",
        "            Z = T @ A_actual + B\n",
        "            f, fp = act_funcion(Z, capa.act_fun)\n",
        "            A_actual = f\n",
        "            Zs[indice] = Z\n",
        "            As[indice] = A_actual\n",
        "            indice += 1\n",
        "\n",
        "        return Zs, As\n",
        "\n",
        "\n",
        "topology = [10, 3, 4, 6, 1]\n",
        "activations = [None, \"sigmoid\", \"sigmoid\", \"sigmoid\", \"sigmoid\"]\n",
        "\n",
        "red = RedManual(topology, activations)\n",
        "\n",
        "xtrain_ = np.random.randn(100, 10)\n",
        "A0 = xtrain_.T\n",
        "\n",
        "Z_dict, A_dict = red.salida_completa(A0)\n",
        "\n",
        "print(\"Dim capa 1:\", Z_dict[1].shape, A_dict[1].shape)\n",
        "print(\"Dim capa 2:\", Z_dict[2].shape, A_dict[2].shape)\n",
        "print(\"Dim capa 3:\", Z_dict[3].shape, A_dict[3].shape)\n",
        "print(\"Dim capa 4:\", Z_dict[4].shape, A_dict[4].shape)\n",
        "\n",
        "print(\"Salida final:\", A_dict[len(topology)-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DApJPrpoP8A7",
        "outputId": "eb2da4ee-26b0-4950-d94f-014da48e5fe7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dim capa 1: (3, 100) (3, 100)\n",
            "Dim capa 2: (4, 100) (4, 100)\n",
            "Dim capa 3: (6, 100) (6, 100)\n",
            "Dim capa 4: (1, 100) (1, 100)\n",
            "Salida final: [[0.81888329 0.8165776  0.82092089 0.81890122 0.8181269  0.81548772\n",
            "  0.81532843 0.81138835 0.81853771 0.81764692 0.81950623 0.82003787\n",
            "  0.81922637 0.8140678  0.81771696 0.81979479 0.81499006 0.81721438\n",
            "  0.81609243 0.81487338 0.81757162 0.81476289 0.81387868 0.81575781\n",
            "  0.8162841  0.81801612 0.8132815  0.81449535 0.8154658  0.81874237\n",
            "  0.81238852 0.81344455 0.81250987 0.82017917 0.81694479 0.81473848\n",
            "  0.81748249 0.81263393 0.8130564  0.81770925 0.81707627 0.81224791\n",
            "  0.81712638 0.81853248 0.81503133 0.81680652 0.81632506 0.8186872\n",
            "  0.81489605 0.81798184 0.81564337 0.81860956 0.81745596 0.81715133\n",
            "  0.8161444  0.81726662 0.81980582 0.818077   0.81751576 0.81351952\n",
            "  0.81261411 0.81355447 0.81705041 0.81586292 0.81463374 0.8137268\n",
            "  0.81627162 0.81303222 0.81114173 0.81469833 0.81559891 0.81942729\n",
            "  0.81578702 0.81928376 0.817182   0.8156634  0.81492065 0.81247903\n",
            "  0.81479213 0.81310055 0.81315973 0.81887903 0.81626657 0.81927529\n",
            "  0.81389873 0.81484091 0.81517585 0.81774067 0.81723478 0.81364936\n",
            "  0.81817909 0.8148594  0.81807655 0.81821081 0.8151942  0.81494203\n",
            "  0.81631132 0.82059004 0.81791792 0.81672463]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El código construye una red neuronal totalmente conectada cuya estructura y funciones de activación se definen mediante una topología suministrada. Cada capa genera una combinación lineal de la entrada usando sus pesos y sesgos, y luego aplica la activación correspondiente para producir la salida que servirá como entrada de la siguiente capa. Durante la propagación hacia adelante, el programa almacena tanto las combinaciones lineales como las salidas activadas de cada nivel, lo que permite examinar la transformación progresiva de los datos desde la entrada original hasta la salida final de la red, obtenida tras atravesar todas las capas."
      ],
      "metadata": {
        "id": "BniUcQ_E3lO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora para la parte 2"
      ],
      "metadata": {
        "id": "rgs3qErkQ8oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def act_function(x, activation):\n",
        "    if activation == \"sigmoid\":\n",
        "        f = lambda t: 1/(1+np.exp(-t))\n",
        "        fp = f(x)*(1-f(x))\n",
        "        return f(x), fp\n",
        "\n",
        "    elif activation == \"tanh\":\n",
        "        f = lambda t: np.tanh(t)\n",
        "        fp = 1 - np.tanh(x)**2\n",
        "        return f(x), fp\n",
        "\n",
        "    elif activation == \"relu\":\n",
        "        f = np.maximum(0, x)\n",
        "        fp = (x > 0).astype(float)\n",
        "        return f, fp\n",
        "\n",
        "    else:\n",
        "        return x, np.ones_like(x)\n",
        "\n",
        "\n",
        "class layer_nn():\n",
        "    def __init__(self, act_fun, nlayer_present, nlayer_before):\n",
        "        self.theta = 2*np.random.random((nlayer_present, nlayer_before)) - 1\n",
        "        self.B = 2*np.random.random((nlayer_present,1)) - 1\n",
        "        self.act_fun = act_fun\n",
        "        self.Z = None\n",
        "        self.A = None\n",
        "\n",
        "    def output(self, Z, A):\n",
        "        self.Z = Z\n",
        "        self.A = A\n",
        "\n",
        "\n",
        "class RedNeuronalGeneralizada:\n",
        "    def __init__(self, topologia, activaciones, semilla=1234):\n",
        "        np.random.seed(semilla)\n",
        "\n",
        "        self.topologia = topologia\n",
        "        self.activaciones = activaciones\n",
        "        self.lista_capas = []\n",
        "        self.numero_capas_totales = len(topologia) - 1\n",
        "\n",
        "        indice = 1\n",
        "        for i in range(1, len(topologia)):\n",
        "            numero_actual = topologia[i]\n",
        "            numero_previo = topologia[i-1]\n",
        "            funcion_act = activaciones[i]\n",
        "            capa = layer_nn(funcion_act, numero_actual, numero_previo)\n",
        "            self.lista_capas.append(capa)\n",
        "            indice += 1\n",
        "\n",
        "\n",
        "    def forward_pass(self, A0):\n",
        "\n",
        "        Z_dict = {}\n",
        "        A_dict = {}\n",
        "\n",
        "        A_actual = A0\n",
        "        A_dict[0] = A_actual\n",
        "\n",
        "        indice_capa = 1\n",
        "\n",
        "        for capa in self.lista_capas:\n",
        "\n",
        "            matriz_pesos = capa.theta\n",
        "            vector_bias = capa.B\n",
        "\n",
        "            Z_actual = np.dot(matriz_pesos, A_actual) + vector_bias\n",
        "\n",
        "            A_activada, derivada_A = act_function(Z_actual, capa.act_fun)\n",
        "\n",
        "            capa.output(Z_actual, A_activada)\n",
        "            Z_dict[indice_capa] = Z_actual\n",
        "            A_dict[indice_capa] = A_activada\n",
        "\n",
        "            A_actual = A_activada\n",
        "            indice_capa += 1\n",
        "\n",
        "        return A_actual, Z_dict, A_dict, self\n",
        "\n",
        "\n",
        "#ejemplo\n",
        "\n",
        "topologia = [5, 4, 3, 2, 1]\n",
        "activaciones = [None, \"sigmoid\", \"tanh\", \"relu\", \"sigmoid\"]\n",
        "\n",
        "red = RedNeuronalGeneralizada(topologia, activaciones)\n",
        "\n",
        "X0 = np.random.randn(5, 20)\n",
        "\n",
        "A_final, Zs, As, red_actualizada = red.forward_pass(X0)\n",
        "\n",
        "print(\"Dimensiones capa 1:\", Zs[1].shape, As[1].shape)\n",
        "print(\"Dimensiones capa 2:\", Zs[2].shape, As[2].shape)\n",
        "print(\"Dimensiones capa 3:\", Zs[3].shape, As[3].shape)\n",
        "print(\"Dimensiones capa 4:\", Zs[4].shape, As[4].shape)\n",
        "print(\"Salida final:\", A_final.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLgU4tpSSYCf",
        "outputId": "0ce84841-7d56-4fe7-f513-c62d75f099c8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensiones capa 1: (4, 20) (4, 20)\n",
            "Dimensiones capa 2: (3, 20) (3, 20)\n",
            "Dimensiones capa 3: (2, 20) (2, 20)\n",
            "Dimensiones capa 4: (1, 20) (1, 20)\n",
            "Salida final: (1, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El código construye una red neuronal multicapa totalmente conectada en la que cada capa posee pesos, sesgos y una función de activación configurable. Durante la propagación hacia adelante, la red toma una entrada inicial y la transforma capa por capa aplicando primero una combinación lineal mediante los pesos y sesgos, y luego una función de activación que determina la salida no lineal de cada nivel. A medida que avanza, el código almacena tanto las combinaciones lineales como las salidas activadas, permitiendo inspeccionar o reutilizar estos valores. Al final, devuelve la salida final de la red junto con los registros internos generados durante el proceso"
      ],
      "metadata": {
        "id": "X3XM7KQk3Ad7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "parte 3."
      ],
      "metadata": {
        "id": "1ZnIM5os2JIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def funcion_coste_binaria(Y, A):\n",
        "    m = Y.shape[1]\n",
        "    acumulado = 0.0\n",
        "    i = 0\n",
        "    while i < m:\n",
        "        y = Y[0, i]\n",
        "        a = A[0, i]\n",
        "        if a <= 0:\n",
        "            a = 0.000000000000001\n",
        "        if a >= 1:\n",
        "            a = 1 - 0.000000000000001\n",
        "        t1 = y * np.log(a)\n",
        "        uno = 1\n",
        "        yinv = uno - y\n",
        "        ainv = uno - a\n",
        "        t2 = yinv * np.log(ainv)\n",
        "        suma = t1 + t2\n",
        "        acumulado = acumulado + suma\n",
        "        i = i + 1\n",
        "    costo = -acumulado / m\n",
        "    return costo\n"
      ],
      "metadata": {
        "id": "XaAx09Vk2c3G"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El código calcula el costo de una clasificación binaria comparando las etiquetas verdaderas con las predicciones generadas por un modelo. Para cada ejemplo, ajusta ligeramente las predicciones extremas para evitar problemas numéricos y luego combina la información de aciertos y errores mediante logaritmos, acumulando el resultado en una suma total. Al finalizar, obtiene el valor promedio negativo de esa suma, que representa la medida final de discrepancia entre lo que el modelo predijo y lo que debía predecir."
      ],
      "metadata": {
        "id": "y7sX64JW3LDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "parte 4."
      ],
      "metadata": {
        "id": "KGGVNsrM2Obz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def act_function(x, activation):\n",
        "    if activation == \"sigmoid\":\n",
        "        f = lambda t: 1/(1+np.exp(-t))\n",
        "        fp = f(x)*(1-f(x))\n",
        "        return f(x), fp\n",
        "    elif activation == \"tanh\":\n",
        "        f = lambda t: np.tanh(t)\n",
        "        fp = 1 - np.tanh(x)**2\n",
        "        return f(x), fp\n",
        "    elif activation == \"relu\":\n",
        "        f = np.maximum(0, x)\n",
        "        fp = (x > 0).astype(float)\n",
        "        return f, fp\n",
        "    else:\n",
        "        uno = np.ones_like(x)\n",
        "        return x, uno\n",
        "\n",
        "class layer_nn():\n",
        "    def __init__(self, act_fun, nlayer_present, nlayer_before):\n",
        "        self.theta = 2*np.random.random((nlayer_present, nlayer_before)) - 1\n",
        "        self.B = 2*np.random.random((nlayer_present,1)) - 1\n",
        "        self.act_fun = act_fun\n",
        "        self.Z = None\n",
        "        self.A = None\n",
        "        self.dZ = None\n",
        "        self.dA = None\n",
        "        self.dtheta = None\n",
        "        self.db = None\n",
        "\n",
        "    def output(self, Z, A):\n",
        "        self.Z = Z\n",
        "        self.A = A\n",
        "\n",
        "class RedNeuronalGeneralizada:\n",
        "    def __init__(self, topologia, activaciones, semilla=1234):\n",
        "        np.random.seed(semilla)\n",
        "        self.topologia = topologia\n",
        "        self.activaciones = activaciones\n",
        "        self.lista_capas = []\n",
        "        self.numero_capas_totales = len(topologia) - 1\n",
        "        i = 1\n",
        "        while i < len(topologia):\n",
        "            nact = topologia[i]\n",
        "            nant = topologia[i-1]\n",
        "            fac = activaciones[i]\n",
        "            c = layer_nn(fac, nact, nant)\n",
        "            self.lista_capas.append(c)\n",
        "            i = i + 1\n",
        "\n",
        "    def forward_pass(self, A0):\n",
        "        self.A0_guardada = A0\n",
        "        Zs = {}\n",
        "        As = {}\n",
        "        Aact = A0\n",
        "        As[0] = Aact\n",
        "        ind = 1\n",
        "        for capa in self.lista_capas:\n",
        "            t = capa.theta\n",
        "            b = capa.B\n",
        "            Znuevo = np.dot(t, Aact) + b\n",
        "            Aactiva, der = act_function(Znuevo, capa.act_fun)\n",
        "            capa.output(Znuevo, Aactiva)\n",
        "            Zs[ind] = Znuevo\n",
        "            As[ind] = Aactiva\n",
        "            Aact = Aactiva\n",
        "            ind = ind + 1\n",
        "        return Aact, Zs, As, self\n",
        "\n",
        "    def backward_pass(self, Y, A_final):\n",
        "        m = Y.shape[1]\n",
        "        L = self.numero_capas_totales\n",
        "        dAL = -(np.divide(Y, A_final) - np.divide(1 - Y, 1 - A_final))\n",
        "        self.lista_capas[L-1].dA = dAL\n",
        "        idx = L-1\n",
        "        while idx >= 0:\n",
        "            capa = self.lista_capas[idx]\n",
        "            Zc = capa.Z\n",
        "            if capa.act_fun == \"sigmoid\":\n",
        "                f = lambda t: 1/(1+np.exp(-t))\n",
        "                fp = f(Zc)*(1-f(Zc))\n",
        "                dZc = capa.dA * fp\n",
        "            elif capa.act_fun == \"tanh\":\n",
        "                fp = 1 - np.tanh(Zc)**2\n",
        "                dZc = capa.dA * fp\n",
        "            elif capa.act_fun == \"relu\":\n",
        "                fp = (Zc > 0).astype(float)\n",
        "                dZc = capa.dA * fp\n",
        "            else:\n",
        "                uno = np.ones_like(Zc)\n",
        "                dZc = capa.dA * uno\n",
        "            capa.dZ = dZc\n",
        "\n",
        "            if idx == 0:\n",
        "                Aantes = self.A0_guardada\n",
        "            else:\n",
        "                Aantes = self.lista_capas[idx-1].A\n",
        "\n",
        "            capa.dtheta = np.dot(dZc, Aantes.T) / m\n",
        "            capa.db = np.sum(dZc, axis=1, keepdims=True) / m\n",
        "\n",
        "            if idx != 0:\n",
        "                tnext = capa.theta\n",
        "                self.lista_capas[idx-1].dA = np.dot(tnext.T, dZc)\n",
        "\n",
        "            idx = idx - 1\n",
        "\n",
        "    def actualizar(self, alfa):\n",
        "        i = 0\n",
        "        while i < len(self.lista_capas):\n",
        "            c = self.lista_capas[i]\n",
        "            c.theta = c.theta - alfa * c.dtheta\n",
        "            c.B = c.B - alfa * c.db\n",
        "            i = i + 1\n",
        "\n",
        "\n",
        "top = [5,4,3,2,1]\n",
        "acts = [None,\"sigmoid\",\"tanh\",\"relu\",\"sigmoid\"]\n",
        "\n",
        "red = RedNeuronalGeneralizada(top, acts)\n",
        "\n",
        "X = np.random.randn(5, 20)\n",
        "Y = (np.random.rand(1,20) > 0.5).astype(float)\n",
        "\n",
        "Afin, Zs, As, r = red.forward_pass(X)\n",
        "red.backward_pass(Y, Afin)\n",
        "red.actualizar(0.01)\n",
        "\n",
        "print(\"proceso terminado\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOdSo0eD2Pw3",
        "outputId": "377331ca-160f-4ba6-c5e9-dbdf81f0b22a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "proceso terminado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El código construye una red neuronal completamente conectada capaz de realizar tanto la propagación hacia adelante como la retropropagación para ajustar sus parámetros. Durante la fase de avance, cada capa transforma las entradas aplicando pesos, sesgos y la función de activación correspondiente, almacenando los valores intermedios para usarlos después. Luego, en la fase de retropropagación, calcula cómo varía el error respecto de cada salida y transmite esa información hacia atrás por todas las capas, obteniendo los gradientes de los pesos y sesgos. A lo último actualiza los parámetros restando una fracción proporcional a esos gradientes, permitiendo que la red aprenda a reducir su error a partir de los datos suministrados."
      ],
      "metadata": {
        "id": "eWvIEr-l3W9z"
      }
    }
  ]
}