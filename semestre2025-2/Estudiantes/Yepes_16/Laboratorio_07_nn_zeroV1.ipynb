{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Sebastian Yepes Acevedo\n",
        "\n",
        "Cc:1007448816"
      ],
      "metadata": {
        "id": "Pdk48KKxTMtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compute_cost(AL, Y, eps=1e-12):\n",
        "    \"\"\"\n",
        "    Coste de entropía cruzada binaria (vectorizado).\n",
        "\n",
        "    AL: salida de la última capa, shape (1, m)\n",
        "    Y : etiquetas verdaderas, shape (1, m)\n",
        "    eps: pequeño valor para estabilidad numérica\n",
        "\n",
        "    Returns:\n",
        "      cost: float escalar\n",
        "    \"\"\"\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    AL_clipped = np.clip(AL, eps, 1 - eps)\n",
        "\n",
        "    # coste\n",
        "    cost = - (1.0 / m) * np.sum(Y * np.log(AL_clipped) + (1 - Y) * np.log(1 - AL_clipped))\n",
        "\n",
        "    return float(cost)\n",
        "\n",
        "\n",
        "def compute_cost_and_dAL(AL, Y, eps=1e-12):\n",
        "    \"\"\"\n",
        "    Devuelve coste y dA_L = dJ/dAL, útil para iniciar backprop.\n",
        "    \"\"\"\n",
        "    m = Y.shape[1]\n",
        "    AL_clipped = np.clip(AL, eps, 1 - eps)\n",
        "\n",
        "    cost = - (1.0 / m) * np.sum(Y * np.log(AL_clipped) + (1 - Y) * np.log(1 - AL_clipped))\n",
        "\n",
        "    # derivada de la función de coste respecto a AL\n",
        "    # dJ/dAL = - (Y/AL) + ((1-Y)/(1-AL))\n",
        "    dAL = - (np.divide(Y, AL_clipped) - np.divide(1 - Y, 1 - AL_clipped)) / m * m\n",
        "    # nota: el /m y *m se cancelan; se deja así para claridad.\n",
        "    # mejor simplificar:\n",
        "    dAL = - (np.divide(Y, AL_clipped) - np.divide(1 - Y, 1 - AL_clipped))\n",
        "\n",
        "    return float(cost), dAL"
      ],
      "metadata": {
        "id": "iPiHg5oP0DUL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se crea una clase de red neuronal, donde se está manejando de entrada la salida de la red neuronal en la utima capa, las etiquetas y el control de errores.\n",
        "\n",
        "Una vez tenemos la base, se crea una función que calcula la función de coste a partir de la ecuación\n",
        "\n",
        "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n",
        "\n",
        "Lueo se obtiene la derivada de la perdida respecto a la salida final $$dA_L=\\frac{\\partial J}{\\partial A^{[L]}}$$\n"
      ],
      "metadata": {
        "id": "SJNv2dUuAXx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(Z):\n",
        "    return np.maximum(0, Z)\n",
        "\n",
        "def sigmoid(Z):\n",
        "    return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "def tanh(Z):\n",
        "    return np.tanh(Z)\n",
        "\n",
        "def relu_backward(dA, Z):\n",
        "    dZ = np.array(dA, copy=True)\n",
        "    dZ[Z <= 0] = 0\n",
        "    return dZ\n",
        "\n",
        "def sigmoid_backward(dA, Z):\n",
        "    s = 1 / (1 + np.exp(-Z))\n",
        "    return dA * s * (1 - s)\n",
        "\n",
        "def tanh_backward(dA, Z):\n",
        "    t = np.tanh(Z)\n",
        "    return dA * (1 - t**2)\n",
        "\n",
        "\n",
        "activation_backward = {\n",
        "    relu: relu_backward,\n",
        "    sigmoid: sigmoid_backward,\n",
        "    tanh: tanh_backward\n",
        "}\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, topology, activations):\n",
        "        \"\"\"\n",
        "        topology   : lista con la cantidad de neuronas por capa [n0, n1, ..., nL]\n",
        "        activations: lista de activaciones [None, act1, act2, ..., actL]\n",
        "        \"\"\"\n",
        "        self.topology = topology\n",
        "        self.activations = activations\n",
        "        self.L = len(topology) - 1  # número de capas no-iniciales\n",
        "\n",
        "        # Inicializamos los parámetros\n",
        "        self.W = {}\n",
        "        self.b = {}\n",
        "\n",
        "        self.Z_cache = {}\n",
        "        self.A_cache = {}\n",
        "\n",
        "        for l in range(1, len(topology)):\n",
        "            n_l = topology[l]\n",
        "            n_prev = topology[l-1]\n",
        "\n",
        "            # Inicialización aleatoria\n",
        "            self.W[l] = np.random.randn(n_l, n_prev) * 0.01\n",
        "            self.b[l] = np.random.randn(n_l, 1) * 0.01\n",
        "\n",
        "    def output(self, X):\n",
        "        \"\"\"\n",
        "        Realiza forward propagation.\n",
        "        X: matriz de entrada de tamaño (n[0], m)\n",
        "\n",
        "        Returns:\n",
        "            Z_list: lista de Z[l]\n",
        "            A_list: lista de A[l] (siendo A[0] = X)\n",
        "        \"\"\"\n",
        "        A = X\n",
        "        A_list = [A]  # A[0]\n",
        "        Z_list = [None]  # Z[0] no existe\n",
        "\n",
        "        for l in range(1, self.L + 1):\n",
        "            Z = self.W[l] @ A + self.b[l]\n",
        "            A = self.activations[l](Z) if self.activations[l] is not None else Z\n",
        "\n",
        "            Z_list.append(Z)\n",
        "            A_list.append(A)\n",
        "\n",
        "        return Z_list, A_list\n",
        "    def forward(self, A0):\n",
        "        \"\"\"\n",
        "        Forward pass interno.\n",
        "        Guarda Z[l] y A[l] en cache.\n",
        "        \"\"\"\n",
        "        self.A_cache[0] = A0\n",
        "        A = A0\n",
        "\n",
        "        for l in range(1, self.L + 1):\n",
        "            Z = self.W[l] @ A + self.b[l]\n",
        "            A = self.activations[l](Z) if self.activations[l] else Z\n",
        "\n",
        "            self.Z_cache[l] = Z\n",
        "            self.A_cache[l] = A\n",
        "\n",
        "        return A\n",
        "\n",
        "\n",
        "    def backward(self, Y):\n",
        "        \"\"\"\n",
        "        Realiza backward propagation completo.\n",
        "\n",
        "        Y: etiquetas reales (1, m)\n",
        "\n",
        "        Returns:\n",
        "            grads: diccionario con dW[l], db[l], dA[l]\n",
        "        \"\"\"\n",
        "        grads = {}\n",
        "        m = Y.shape[1]\n",
        "\n",
        "        # Recuperamos la salida final A[L]\n",
        "        AL = self.A_cache[self.L]\n",
        "\n",
        "        # Paso 1: dAL (derivada de la pérdida)\n",
        "        dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "\n",
        "        # Paso 2: capa L\n",
        "        ZL = self.Z_cache[self.L]\n",
        "        act_L = self.activations[self.L]\n",
        "\n",
        "        dZL = activation_backward[act_L](dAL, ZL)\n",
        "        dWL = (1/m) * (dZL @ self.A_cache[self.L - 1].T)\n",
        "        dbL = (1/m) * np.sum(dZL, axis=1, keepdims=True)\n",
        "        dA_prev = self.W[self.L].T @ dZL\n",
        "\n",
        "        grads[f\"dW{self.L}\"] = dWL\n",
        "        grads[f\"db{self.L}\"] = dbL\n",
        "\n",
        "        # ---- Paso 3: capas L-1 ... 1\n",
        "        for l in reversed(range(1, self.L)):\n",
        "            Z = self.Z_cache[l]\n",
        "            A_prev = self.A_cache[l-1]\n",
        "            act = self.activations[l]\n",
        "\n",
        "            dZ = activation_backward[act](dA_prev, Z)\n",
        "            dW = (1/m) * (dZ @ A_prev.T)\n",
        "            db = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "            dA_prev = self.W[l].T @ dZ\n",
        "\n",
        "            grads[f\"dW{l}\"] = dW\n",
        "            grads[f\"db{l}\"] = db\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def compute_cost(self, Y, eps=1e-12):\n",
        "        \"\"\"\n",
        "        Calcula el coste usando la salida guardada en A_cache[L].\n",
        "        Y: etiquetas shape (1, m)\n",
        "        \"\"\"\n",
        "        AL = self.A_cache[self.L]  # salida final\n",
        "        return compute_cost(AL, Y, eps)\n",
        "\n",
        "    def compute_cost_and_dAL(self, Y, eps=1e-12):\n",
        "        AL = self.A_cache[self.L]\n",
        "        return compute_cost_and_dAL(AL, Y, eps)"
      ],
      "metadata": {
        "id": "Lmjda6wqyCwg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se definen las funciones de activación necesarias para cada uno de las funciones de la clase.\n",
        "\n",
        "En este caso se recibe la topología y las activaciones por capa. Se inicializan los pesos de forma aleatoria y luego se comienza con cada uno de los métodos.\n",
        "\n",
        "**Output**: Calcula el forward propagation y devuelve listas con Z y A de cada capa.\n",
        "\n",
        "**Forward**: Realiza la propagación guardando en Z y A.\n",
        "\n",
        "**Backward**: Calcula la derivada del costo respecto a AL, se aplica en cada capa y devuelve un diccionario.\n",
        "\n",
        "Nuevamente se obtiene la función de costo para que la clase sea más completa.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iDAlXo2SCTGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(A0, nn_red):\n",
        "    \"\"\"\n",
        "    A0: entrada inicial (X)\n",
        "    nn_red: objeto de tipo NeuralNetwork\n",
        "\n",
        "    Return:\n",
        "      - salida final A\n",
        "      - red actualizada (nn_red)\n",
        "    \"\"\"\n",
        "    A = nn_red.forward(A0)\n",
        "    return A, nn_red"
      ],
      "metadata": {
        "id": "xgj5LbZpy1iX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algunos ejemplos para comprobar el funcionamiento de la clase."
      ],
      "metadata": {
        "id": "fkW0xm0OEV3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topology = [3, 4, 2, 1]\n",
        "activations = [None, relu, relu, sigmoid]\n",
        "\n",
        "nn = NeuralNetwork(topology, activations)\n",
        "\n",
        "# Entrada inicial (3 características, 5 ejemplos)\n",
        "A0 = np.random.randn(3, 5)\n",
        "\n",
        "A_final, nn_actualizada = forward_pass(A0, nn)\n",
        "\n",
        "print(\"Salida final:\")\n",
        "print(A_final)\n",
        "\n",
        "print(\"\\nDimensión Z[2]:\", nn_actualizada.Z_cache[2].shape)\n",
        "print(\"Dimensión A[2]:\", nn_actualizada.A_cache[2].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuN1Put4y47t",
        "outputId": "4fee14a6-a678-438c-bff9-43c0fb7f0760"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Salida final:\n",
            "[[0.5034157  0.50341502 0.50341604 0.50341522 0.5034155 ]]\n",
            "\n",
            "Dimensión Z[2]: (2, 5)\n",
            "Dimensión A[2]: (2, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nn.A_cache[L] contiene AL con shape (1, m)\n",
        "Y = np.array([[1, 0, 1, 1, 0]])\n",
        "AL = nn.A_cache[nn.L]         # salida (1,5)\n",
        "\n",
        "cost = nn.compute_cost(Y)\n",
        "cost2, dAL = nn.compute_cost_and_dAL(Y)\n",
        "\n",
        "print(\"Coste:\", cost)\n",
        "print(\"dAL shape:\", dAL.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fthVVXEGzTIZ",
        "outputId": "24f21cbb-c109-4e20-a97f-da48c7c26b6a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coste: 0.6918039169523441\n",
            "dAL shape: (1, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Forward pass\n",
        "AL, nn = forward_pass(A0, nn)\n",
        "\n",
        "# 2. Backward pass\n",
        "grads = nn.backward(Y)\n",
        "\n",
        "print(\"Gradiente de W2:\", grads[\"dW2\"])\n",
        "print(\"Gradiente de b2:\", grads[\"db2\"])\n"
      ],
      "metadata": {
        "id": "5moA2iI11MHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffca2636-92bd-4cef-82b1-236ba0acff7e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradiente de W2: [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [-1.25291262e-05  4.11668539e-05 -2.82864099e-05 -1.18780094e-05]]\n",
            "Gradiente de b2: [[0.        ]\n",
            " [0.00135036]]\n"
          ]
        }
      ]
    }
  ]
}