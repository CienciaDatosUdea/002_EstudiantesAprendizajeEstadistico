{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from utils import ising_data_builder, H5IsingDataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Red Neuronal Fully Connected para Clasificación del Modelo Ising\n",
        "\n",
        "Este notebook entrena y evalúa una red neuronal fully connected para clasificar las fases del modelo Ising.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construcción del Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el dataset\n",
        "data = ising_data_builder('../data/').h5_path\n",
        "dataset = H5IsingDataset(data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## División Train/Test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = len(dataset)\n",
        "labels = np.asarray(dataset.y[:]) \n",
        "\n",
        "# Train / Test Split estratificado\n",
        "train_idx, test_idx = train_test_split(\n",
        "    np.arange(n),\n",
        "    test_size=0.2,\n",
        "    stratify=labels,\n",
        "    random_state=4\n",
        ")\n",
        "\n",
        "# Subsets que no copian datos, solo crean vistas por índices\n",
        "train_set = Subset(dataset, train_idx.tolist())\n",
        "test_set = Subset(dataset, test_idx.tolist())\n",
        "\n",
        "# DataLoaders -> genera los batches\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definición del Modelo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IsingNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Topología de la red \n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),          # Aplana el tensor a (batch, N)\n",
        "            nn.Linear(input_dim, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 1)\n",
        "            # Nota: No aplicamos Sigmoid aquí porque BCEWithLogitsLoss lo hace internamente\n",
        "            # de forma más estable numéricamente\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Model, loss, optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = IsingNN(input_dim=100).to(device)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Funciones de Entrenamiento y Evaluación\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenamiento\n",
        "def train_epoch(model, loader):\n",
        "    model.train()\n",
        "    total_loss, total_correct, total = 0, 0, 0\n",
        "\n",
        "    for X, y in loader:\n",
        "        X, y = X.to(device), y.float().to(device)\n",
        "\n",
        "        logits = model(X).squeeze()  # Forward y eliminar dimensión extra [batch, 1] -> [batch]\n",
        "        loss = loss_fn(logits, y)    # Loss\n",
        "\n",
        "        optimizer.zero_grad() # Limpiar gradientes\n",
        "        loss.backward()       # Backward propagation\n",
        "        optimizer.step()      # Update weights and biases\n",
        "\n",
        "        total_loss += loss.item() * X.size(0) # Pérdida del batch\n",
        "\n",
        "        preds = (torch.sigmoid(logits) >= 0.5).long()     # Logits a labels\n",
        "        total_correct += (preds == y.long()).sum().item() # Contar correctos\n",
        "        total += X.size(0) \n",
        "        \n",
        "    # Pérdida promedio y accuracy\n",
        "    return total_loss / total, total_correct / total \n",
        "\n",
        "# Evaluación\n",
        "def eval_epoch(model, loader):\n",
        "    model.eval()\n",
        "    total_loss, total_correct, total = 0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in loader:\n",
        "            X, y = X.to(device), y.float().to(device)\n",
        "\n",
        "            logits = model(X).squeeze()  # Eliminar dimensión extra [batch, 1] -> [batch]\n",
        "            loss = loss_fn(logits, y)\n",
        "\n",
        "            total_loss += loss.item() * X.size(0)\n",
        "            preds = (torch.sigmoid(logits) >= 0.5).long()\n",
        "            total_correct += (preds == y.long()).sum().item()\n",
        "            total += X.size(0)\n",
        "\n",
        "    return total_loss / total, total_correct / total\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loop de entrenamiento\n",
        "for epoch in tqdm(range(1, 6), desc=\"Epochs\"):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader)\n",
        "    test_loss, test_acc = eval_epoch(model, test_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch} | \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
        "          f\"Test Loss: {test_loss:.4f}, Acc: {test_acc:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Matriz de Confusión\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X, y in test_loader:\n",
        "        X = X.to(device)\n",
        "\n",
        "        logits = model(X)\n",
        "\n",
        "        # Convertir a clases 0/1\n",
        "        preds = (torch.sigmoid(logits) >= 0.5).long()\n",
        "\n",
        "        # Guardar todo\n",
        "        all_preds.append(preds.cpu())\n",
        "        all_labels.append(y.long().cpu())\n",
        "\n",
        "# Convertir listas de tensores a arrays numpy concatenados\n",
        "all_preds = torch.cat(all_preds).numpy() \n",
        "all_labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "# Matriz de confusión\n",
        "cm = confusion_matrix(all_labels, all_preds, normalize='true')\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicción\")\n",
        "plt.ylabel(\"Real\")\n",
        "plt.title(\"Matriz de Confusión\")\n",
        "plt.tight_layout()\n",
        "plt.savefig('../img_results/confusion_matrix_nn.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accuracy por Temperatura\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análisis de accuracy por temperatura\n",
        "import os\n",
        "sys.path.append('..')\n",
        "from utils import read_ising_file\n",
        "\n",
        "model.eval()\n",
        "temp_data = defaultdict(lambda: {'preds': [], 'labels': []})\n",
        "folder = '../data/'\n",
        "\n",
        "# Procesar archivos y hacer predicciones por lotes\n",
        "for file in os.listdir(folder):\n",
        "    if 'ising_' not in file:\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        ising = read_ising_file(folder + file)\n",
        "        temp = ising.metadata['T']\n",
        "        label = ising.metadata['class']\n",
        "        spins = ising.load_all_spins(invert=False)\n",
        "        \n",
        "        # Procesar todos los espines de un archivo en un solo batch\n",
        "        with torch.no_grad():\n",
        "            X = torch.tensor(spins, dtype=torch.float32).to(device)\n",
        "            preds = (torch.sigmoid(model(X)) >= 0.5).long().cpu().numpy()\n",
        "            \n",
        "            temp_data[temp]['preds'].extend(preds)\n",
        "            temp_data[temp]['labels'].extend([label] * len(preds))\n",
        "    except (ValueError, KeyError, IndexError):\n",
        "        continue\n",
        "\n",
        "# Calcular accuracy por temperatura\n",
        "temperaturas, accuracies = [], []\n",
        "for temp in sorted(temp_data.keys()):\n",
        "    preds = np.array(temp_data[temp]['preds'])\n",
        "    labels = np.array(temp_data[temp]['labels'])\n",
        "    accuracies.append(np.mean(preds == labels))\n",
        "    temperaturas.append(temp)\n",
        "\n",
        "temperaturas, accuracies = np.array(temperaturas), np.array(accuracies)\n",
        "\n",
        "# Graficar\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(temperaturas, accuracies, marker='o', linestyle='-', linewidth=2, markersize=8, color='#2E86AB')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.xlabel('Temperatura', fontsize=12)\n",
        "plt.ylabel('Accuracy Promedio', fontsize=12)\n",
        "plt.title('Accuracy del Modelo vs Temperatura', fontsize=14, fontweight='bold')\n",
        "plt.ylim([0, 1.05])\n",
        "plt.tight_layout()\n",
        "plt.savefig('../img_results/accuracy_vs_temperature_nn.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Mostrar resultados\n",
        "print(\"\\nAccuracy por temperatura:\")\n",
        "print(\"-\" * 40)\n",
        "for temp, acc in zip(temperaturas, accuracies):\n",
        "    print(f\"T = {temp:.3f}: {acc:.4f} ({acc*100:.2f}%)\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
