{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/CienciaDatosUdea/002_EstudiantesAprendizajeEstadistico/blob/main/semestre2026-1/Sesiones/Sesion_02a_repaso_estadistica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNcYxoCWjo8x"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estadística Básica\n",
    "## Características de densidades de probabilidad\n",
    "### Probabilidades unidimensionales: media, varianza, sesgo, kurtosis\n",
    "REf: Sirca cap 4.\n",
    "\n",
    "\n",
    "Sea $X$ una variable aleatoria (v.a.) unidimensional que sigue una densidad f. Su esperanza o valor medio se escribe como: \n",
    "\n",
    "\n",
    "$$E[X] = \\bar{X} = \\int_{-\\infty}^{+\\infty} x f(x) \\, dx$$\n",
    "\n",
    "que se generaliza con el concepto de p-esimo momento de la v.a:\n",
    "\n",
    "$$M'_p = E[X^p] = \\int_{-\\infty}^{+\\infty} x^p f_x(x) \\, dx$$\n",
    "\n",
    "y el p-esimo momento centralizado:\n",
    "\n",
    "$$M_p = E[(X - \\bar{X})^p]$$\n",
    "\n",
    "Los primeros cuatro momentos se usan en estadística para describir de manera muy informativa las características de una densidad de probabilidad.\n",
    "$$\n",
    "M'_1 = \\bar{X} \\quad \\text{promedio o media}\n",
    "$$\n",
    "$$\n",
    "M_2 = \\sigma^2 \\quad \\text{varianza}\n",
    "$$\n",
    "$$\n",
    "\\rho = \\frac{M_3}{\\sigma^3} \\quad \\text{skewness o sesgo} \\quad \\text{(asimetría respecto a } \\bar{X} \\text{)}\n",
    "$$\n",
    "$$\n",
    "\\varepsilon = \\frac{M_4}{\\sigma^4} - 3 \\quad \\text{exceso de kurtosis} \\quad \\text{(desviación a una gausiana)}\n",
    "$$\n",
    "![](Figuras/distribuciones.png)\n",
    "\n",
    "\n",
    "### Variables Aleatorias Conjuntas: Momentos y Covarianza\n",
    "Sean $X$, $Y$ dos variables aleatorias con densidad conjunta $f_{X,Y}(x,y)$.\n",
    "Los momentos de primer orden se obtienen integrando cada variable frente a la\n",
    "densidad conjunta. Las medias de $X$ e $Y$ son, respectivamente:\n",
    "$$\n",
    "\\bar{X} = E[X] = \\int_{-\\infty}^{+\\infty}\\!\\int_{-\\infty}^{+\\infty}\n",
    "x\\, f_{X,Y}(x,y)\\,dx\\,dy\n",
    "$$\n",
    "$$\n",
    "\\bar{Y} = E[Y] = \\int_{-\\infty}^{+\\infty}\\!\\int_{-\\infty}^{+\\infty}\n",
    "y\\, f_{X,Y}(x,y)\\,dx\\,dy\n",
    "$$\n",
    "Las varianzas miden la dispersión de cada variable en torno a su media:\n",
    "$$\n",
    "\\sigma_X^2 = E\\!\\left[(X-\\bar{X})^2\\right]\n",
    "= \\int_{-\\infty}^{+\\infty}\\!\\int_{-\\infty}^{+\\infty}\n",
    "(x-\\bar{X})^2\\,f_{X,Y}(x,y)\\,dx\\,dy\n",
    "$$\n",
    "$$\n",
    "\\sigma_Y^2 = E\\!\\left[(Y-\\bar{Y})^2\\right]\n",
    "= \\int_{-\\infty}^{+\\infty}\\!\\int_{-\\infty}^{+\\infty}\n",
    "(y-\\bar{Y})^2\\,f_{X,Y}(x,y)\\,dx\\,dy\n",
    "$$\n",
    "#### Covarianza.\n",
    "Mientras que las varianzas describen el comportamiento individual de cada\n",
    "variable, la covarianza cuantifica cómo varían conjuntamente. Se define como\n",
    "el momento mixto centrado:\n",
    "$$\n",
    "\\sigma_{XY} = \\mathrm{cov}[X,Y]\n",
    "= E\\!\\left[(X-\\bar{X})(Y-\\bar{Y})\\right]\n",
    "= \\int_{-\\infty}^{+\\infty}\\!\\int_{-\\infty}^{+\\infty}\n",
    "(x-\\bar{X})(y-\\bar{Y})\\,f_{X,Y}(x,y)\\,dx\\,dy\n",
    "$$\n",
    "Un valor positivo indica que $X$ e $Y$ tienden a crecer juntas; un valor\n",
    "negativo, que se mueven en sentidos opuestos; y un valor nulo señala ausencia\n",
    "de relación lineal. La covarianza es la base del coeficiente de correlación\n",
    "de Pearson.\n",
    "\n",
    "#### Propiedad: forma alternativa de la covarianza.\n",
    "Expandiendo el producto $(X-\\bar{X})(Y-\\bar{Y})$ y aplicando linealidad de\n",
    "la esperanza, se obtiene una expresión equivalente más compacta. Desarrollando\n",
    "el producto y usando que $\\bar{X}$ y $\\bar{Y}$ son constantes:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma_{XY}\n",
    "&= E\\!\\left[XY - \\bar{Y}X - \\bar{X}Y + \\bar{X}\\bar{Y}\\right] \\\\\n",
    "&= E[XY] - \\bar{Y}\\,E[X] - \\bar{X}\\,E[Y] + \\bar{X}\\bar{Y} \\\\\n",
    "&= E[XY] - \\bar{X}\\bar{Y}\n",
    "\\end{aligned}\n",
    "$$\n",
    "La covarianza es pues la diferencia entre el momento conjunto $E[XY]$ y el\n",
    "producto de las medias. Esta forma es especialmente útil en la práctica.\n",
    "\n",
    "#### Propiedad: independencia implica covarianza nula.\n",
    "Si $X$ e $Y$ son independientes, la covarianza se anula:\n",
    "$$\n",
    "X, Y \\text{ independientes} \\implies \\sigma_{XY} = 0\n",
    "$$\n",
    "Es importante notar que el recíproco \\emph{no} es cierto en general: covarianza\n",
    "nula es condición necesaria pero no suficiente para la independencia.\n",
    "\n",
    "#### Demostración.\n",
    "Cuando $X$ e $Y$ son independientes, la esperanza del producto factoriza:\n",
    "$E[XY] = E[X]\\,E[Y] = \\bar{X}\\bar{Y}$. Sustituyendo en la forma alternativa:\n",
    "$$\n",
    "\\sigma_{XY} = E[XY] - \\bar{X}\\bar{Y} = \\bar{X}\\bar{Y} - \\bar{X}\\bar{Y} = 0\n",
    "$$\n",
    "#### Varianza de la suma.\n",
    "La varianza de la suma de dos variables aleatorias no es en general la suma\n",
    "de sus varianzas; aparece un término correctivo que recoge su dependencia:\n",
    "$$\n",
    "\\mathrm{var}[X+Y] = \\mathrm{var}[X] + \\mathrm{var}[Y] + 2\\,\\mathrm{cov}[X,Y]\n",
    "$$\n",
    "Cuando $X$ e $Y$ son independientes, $\\mathrm{cov}[X,Y]=0$ y la varianza de\n",
    "la suma sí coincide con la suma de varianzas.\n",
    "\n",
    "#### Demostración.\n",
    "Aplicamos la definición de varianza a $X+Y$, usando que\n",
    "$E[X+Y] = \\bar{X}+\\bar{Y}$, y expandimos el cuadrado del binomio:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathrm{var}[X+Y]\n",
    "&= E\\!\\left[\\bigl((X+Y)-(\\bar{X}+\\bar{Y})\\bigr)^2\\right] \\\\\n",
    "&= E\\!\\left[\\bigl((X-\\bar{X})+(Y-\\bar{Y})\\bigr)^2\\right] \\\\\n",
    "&= E\\!\\left[(X-\\bar{X})^2\\right]\n",
    " + 2\\,E\\!\\left[(X-\\bar{X})(Y-\\bar{Y})\\right]\n",
    " + E\\!\\left[(Y-\\bar{Y})^2\\right] \\\\\n",
    "&= \\mathrm{var}[X] + \\mathrm{var}[Y] + 2\\,\\mathrm{cov}[X,Y]\n",
    "\\qquad \\blacksquare\n",
    "\\end{aligned}\n",
    "$$\n",
    "#### Generalización a $n$ variables.\n",
    "El mismo razonamiento se extiende a una suma arbitraria de $n$ variables\n",
    "aleatorias. Los términos diagonales aportan las varianzas individuales, y\n",
    "los términos cruzados aportan todas las covarianzas entre pares distintos:\n",
    "$$\n",
    "\\mathrm{var}\\!\\left[\\sum_{i=1}^n X_i\\right]\n",
    "= \\sum_{i=1}^{n} \\mathrm{var}[X_i]\n",
    "+ 2\\sum_{i < j} \\mathrm{cov}[X_i, X_j]\n",
    "$$\n",
    "Si todas las variables son independientes entre sí, los términos de covarianza\n",
    "se anulan y la varianza de la suma es simplemente la suma de las varianzas\n",
    "individuales.\n",
    "\n",
    "\n",
    "### Coeficiente de Correlación Lineal de Pearson\n",
    "El coeficiente de correlación lineal de Pearson entre dos variables aleatorias $X$ e $Y$ se define como la covarianza normalizada por el producto de sus desviaciones estándar:\n",
    "\n",
    "$$\\rho_{X,Y} = \\frac{\\sigma_{X,Y}}{\\sigma_X \\sigma_Y}$$\n",
    "\n",
    "Este coeficiente toma valores en el intervalo $[-1, 1]$, donde $|\\rho_{X,Y}| = 1$ indica una dependencia lineal perfecta entre las variables.\n",
    "#### Demostración: $|\\rho_{X,Y}| \\leq 1$ (Brémaud)\n",
    "Para demostrar la acotación del coeficiente, introducimos una variable auxiliar parametrizada. Sea $\\lambda \\in \\mathbb{R}$ un parámetro real arbitrario y definamos:\n",
    "$$\n",
    "Z_\\lambda = (X - \\mu_X) + \\lambda\\,(Y - \\mu_Y)\n",
    "$$\n",
    "Al ser $Z_\\lambda$ una variable aleatoria real, su segundo momento es necesariamente no negativo:\n",
    "$$\n",
    "E[Z_\\lambda^2] \\geq 0\n",
    "$$\n",
    "Expandiendo el cuadrado y aplicando linealidad de la esperanza, se obtiene:\n",
    "$$\n",
    "E[Z_\\lambda^2] = \\sigma_X^2 + 2\\lambda\\,\\sigma_{X,Y} + \\lambda^2\\,\\sigma_Y^2 \\geq 0\n",
    "$$\n",
    "Esta expresión es un polinomio de segundo grado en $\\lambda$ con coeficiente principal $\\sigma_Y^2 > 0$. Para que sea no negativo para todo $\\lambda \\in \\mathbb{R}$, su discriminante debe ser menor o igual a cero:\n",
    "$$\n",
    "\\Delta = 4\\sigma_{X,Y}^2 - 4\\sigma_X^2\\,\\sigma_Y^2 \\leq 0\n",
    "$$\n",
    "Dividiendo entre 4 y reordenando:\n",
    "$$\n",
    "\\sigma_{X,Y}^2 \\leq \\sigma_X^2\\,\\sigma_Y^2\n",
    "$$\n",
    "Dividiendo ambos lados por $\\sigma_X^2\\,\\sigma_Y^2 > 0$ y tomando raíz cuadrada se obtiene la cota buscada:\n",
    "$$\n",
    "\\left(\\frac{\\sigma_{X,Y}}{\\sigma_X\\,\\sigma_Y}\\right)^2 \\leq 1 \\implies |\\rho_{X,Y}| \\leq 1\n",
    "$$\n",
    "#### Condición de igualdad: $|\\rho_{X,Y}| = 1$\n",
    "La igualdad $|\\rho_{X,Y}| = 1$ se alcanza si y solo si $\\Delta = 0$, lo que significa que el polinomio en $\\lambda$ tiene una única raíz real $\\lambda_0$:\n",
    "$$\n",
    "\\lambda_0 = -\\frac{\\sigma_{X,Y}}{\\sigma_Y^2}\n",
    "$$\n",
    "En ese caso, el polinomio se anula en $\\lambda_0$, es decir:\n",
    "$$\n",
    "E[Z_{\\lambda_0}^2] = 0\n",
    "$$\n",
    "Dado que $Z_{\\lambda_0}^2 \\geq 0$ casi seguramente y su esperanza es cero, la variable debe ser nula con probabilidad uno:\n",
    "$$\n",
    "P(Z_{\\lambda_0} = 0) = 1\n",
    "$$\n",
    "Sustituyendo la definición de $Z_{\\lambda_0}$, esto equivale a la existencia de una relación lineal exacta entre $X$ e $Y$, válida casi seguramente:\n",
    "$$\n",
    "(X - \\mu_X) + \\lambda_0\\,(Y - \\mu_Y) = 0 \\quad \\text{c.s.}\n",
    "$$\n",
    "Es decir, existe una constante $\\lambda_0 \\neq 0$ tal que $X - \\mu_X = -\\lambda_0\\,(Y - \\mu_Y)$ casi seguramente, lo que confirma que la correlación unitaria es equivalente a una dependencia lineal perfecta entre las variables.\n",
    "\n",
    "\n",
    "## Teoremas de de límites\n",
    "### Desigualdad de Markov\n",
    "La desigualdad de Markov establece una cota superior para la probabilidad de que\n",
    "una variable aleatoria no negativa supere un umbral dado. Para cualquier función\n",
    "medible $f$ y constante $a > 0$:\n",
    "$$\n",
    "P\\!\\left(|f(X)| \\geq a\\right) \\leq \\frac{E\\!\\left[|f(X)|\\right]}{a}\n",
    "$$\n",
    "#### Demostración.\n",
    "Partimos de la esperanza de $|f(X)|$, que podemos minorizar restringiendo la\n",
    "integral al conjunto donde $|f(X)| \\geq a$:\n",
    "$$\n",
    "E\\!\\left[|f(X)|\\right]\n",
    "= \\int_{-\\infty}^{\\infty} |f(x)|\\, p(x)\\,dx\n",
    "\\geq \\int_{\\{|f(x)| \\geq a\\}} |f(x)|\\, p(x)\\,dx\n",
    "$$\n",
    "Sobre ese conjunto, $|f(x)| \\geq a$, por lo que podemos sustituir el integrando\n",
    "por la cota inferior $a$:\n",
    "$$\n",
    "\\geq \\int_{\\{|f(x)| \\geq a\\}} a\\, p(x)\\,dx\n",
    "= a\\, P\\!\\left(|f(X)| \\geq a\\right)\n",
    "$$\n",
    "Dividiendo ambos lados entre $a > 0$ se obtiene la desigualdad de Markov:\n",
    "$$\n",
    "P\\!\\left(|f(X)| \\geq a\\right) \\leq \\frac{E\\!\\left[|f(X)|\\right]}{a} \n",
    "$$\n",
    "### Desigualdad de Chebyshev\n",
    "La desigualdad de Chebyshev se obtiene aplicando Markov con una elección\n",
    "particular de $f$ y $a$. Sea $X$ una variable aleatoria con media $m = E[X]$\n",
    "y varianza $\\sigma^2 = E[(X-m)^2] < \\infty$. Tomamos:\n",
    "$$\n",
    "f(X) = (X - m)^2 \\qquad \\text{y} \\qquad a = \\varepsilon^2, \\quad \\varepsilon > 0\n",
    "$$\n",
    "Con esta elección, la esperanza de $f(X)$ es precisamente la varianza:\n",
    "$$\n",
    "E\\!\\left[|f(X)|\\right] = E\\!\\left[(X-m)^2\\right] = \\sigma^2\n",
    "$$\n",
    "Sustituyendo en la desigualdad de Markov, el evento $\\{f(X) \\geq \\varepsilon^2\\}$\n",
    "es idéntico al evento $\\{|X - m| \\geq \\varepsilon\\}$, ya que ambas expresiones\n",
    "son equivalentes al elevar al cuadrado:\n",
    "$$\n",
    "P\\!\\left(|X - m|^2 \\geq \\varepsilon^2\\right) \\leq \\frac{\\sigma^2}{\\varepsilon^2}\n",
    "$$\n",
    "Dado que $|X - m| \\geq 0$, los dos eventos son el mismo conjunto, y se obtiene\n",
    "la desigualdad de Chebyshev:\n",
    "$$\n",
    "P\\!\\left(|X - m| \\geq \\varepsilon\\right) \\leq \\frac{\\sigma^2}{\\varepsilon^2},\n",
    "\\qquad \\varepsilon > 0\n",
    "$$\n",
    "Esta cota es universal: solo requiere conocer la varianza, sin ningún supuesto\n",
    "sobre la distribución de $X$. Cuanto mayor es $\\varepsilon$ o menor es $\\sigma^2$,\n",
    "más estrecha resulta la cota.\n",
    "\n",
    "### Ley Débil de los Grandes Números\n",
    "#### Enunciado. Sea $(X_n)_{n \\geq 1}$ una sucesión de variables aleatorias\n",
    "independientes e idénticamente distribuidas (i.i.d.) con media $m = E[X_i]$ y\n",
    "varianza $\\sigma^2 = \\mathrm{Var}(X_i) < \\infty$. Definimos la \\emph{media empírica}:\n",
    "$$\n",
    "\\bar{X}_n = \\frac{S_n}{n} = \\frac{X_1 + \\cdots + X_n}{n}\n",
    "$$\n",
    "#### Propiedades de la media empírica.\n",
    "Por linealidad de la esperanza y por la independencia de las variables, la media\n",
    "empírica tiene media y varianza:\n",
    "$$\n",
    "E\\!\\left[\\bar{X}_n\\right] = m\n",
    "\\qquad \\text{y} \\qquad\n",
    "\\mathrm{Var}\\!\\left(\\bar{X}_n\\right) = \\frac{\\sigma^2}{n}\n",
    "$$\n",
    "La varianza tiende a cero conforme $n \\to \\infty$, lo que sugiere que $\\bar{X}_n$\n",
    "se concentra en torno a $m$. Aplicando la desigualdad de Chebyshev a $\\bar{X}_n$:\n",
    "$$\n",
    "P\\!\\left(\\left|\\bar{X}_n - m\\right| \\geq \\varepsilon\\right)\n",
    "\\leq \\frac{\\mathrm{Var}(\\bar{X}_n)}{\\varepsilon^2}\n",
    "= \\frac{\\sigma^2}{n\\,\\varepsilon^2}\n",
    "$$\n",
    "El lado derecho tiende a cero para cualquier $\\varepsilon > 0$ fijo cuando\n",
    "$n \\to \\infty$. Tomando el límite:\n",
    "$$\n",
    "\\lim_{n \\to \\infty} P\\!\\left(\\left|\\bar{X}_n - m\\right| \\geq \\varepsilon\\right) = 0\n",
    "\\qquad \\forall\\, \\varepsilon > 0\n",
    "$$\n",
    "Esto es precisamente la definición de convergencia en probabilidad, y constituye\n",
    "la \\textbf{Ley Débil de los Grandes Números}:\n",
    "$$\n",
    "\\frac{S_n}{n} \\xrightarrow{\\,P\\,} m \\qquad (n \\to \\infty)\n",
    "$$\n",
    "La media empírica converge en probabilidad a la media teórica $m$: para muestras\n",
    "suficientemente grandes, la probabilidad de que $\\bar{X}_n$ se aleje de $m$ más\n",
    "de cualquier margen $\\varepsilon$ es arbitrariamente pequeña.\n",
    "\n",
    "\n",
    "### Teorema del Límite Central\n",
    "### Funciones Generadoras de Momentos\n",
    "La función generadora de momentos (FGM) de una variable aleatoria $X$ codifica\n",
    "toda la información sobre sus momentos en una única función. Para el caso\n",
    "continuo se define como:\n",
    "$$\n",
    "M_X(t) = E\\!\\left[e^{tX}\\right] = \\int_{-\\infty}^{\\infty} e^{tx}\\, f_X(x)\\, dx\n",
    "$$\n",
    "y en el caso discreto la integral se reemplaza por una suma sobre los valores\n",
    "posibles de $X$:\n",
    "$$\n",
    "M_X(t) = \\sum_i e^{t x_i}\\, P(X = x_i)\n",
    "$$\n",
    "#### Conexión con los momentos.\n",
    "El interés de la FGM reside en que, al expandir $e^{tX}$ en serie de Taylor\n",
    "alrededor de $t = 0$ e intercambiar la esperanza con la suma, todos los momentos\n",
    "de $X$ aparecen como coeficientes:\n",
    "$$\n",
    "M_X(t)\n",
    "= E\\!\\left[\\sum_{k=0}^{\\infty} \\frac{t^k X^k}{k!}\\right]\n",
    "= 1 + t\\,E[X] + \\frac{t^2}{2}\\,E[X^2] + \\frac{t^3}{3!}\\,E[X^3] + \\cdots\n",
    "$$\n",
    "Esto permite recuperar cualquier momento por diferenciación. Derivando respecto\n",
    "a $t$ bajo el signo de esperanza:\n",
    "$$\n",
    "\\frac{d}{dt} M_X(t) = E\\!\\left[X e^{tX}\\right]\n",
    "$$\n",
    "Evaluando en $t = 0$ todos los términos de la serie se anulan salvo el primero,\n",
    "entregando la media:\n",
    "$$\n",
    "\\left.\\frac{d}{dt} M_X(t)\\right|_{t=0} = E[X]\n",
    "$$\n",
    "En general, la $r$-ésima derivada evaluada en $t = 0$ proporciona el momento\n",
    "de orden $r$:\n",
    "$$\n",
    "E[X^r] = \\left.\\frac{d^r}{dt^r} M_X(t)\\right|_{t=0}, \\qquad r = 0, 1, 2, \\ldots\n",
    "$$\n",
    "Esta propiedad convierte a la FGM en una herramienta central para calcular\n",
    "momentos sin necesidad de integrar directamente.\n",
    "\n",
    "\n",
    "#### Caso de la distribución normal estándar $$\\mathcal{N (0,1)}$$\n",
    "Para una variable $X \\sim \\mathcal{N}(0,1)$, sustituimos su densidad en la\n",
    "definición de la FGM:\n",
    "$$\n",
    "M_X(t)\n",
    "= \\int_{-\\infty}^{\\infty} e^{tx}\\,\\frac{1}{\\sqrt{2\\pi}}\\,e^{-\\frac{x^2}{2}}\\,dx\n",
    "= \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} e^{-\\frac{x^2}{2} + tx}\\,dx\n",
    "$$\n",
    "Reconocemos la forma cuadrática en el exponente y aplicamos la identidad\n",
    "gaussiana con $a = \\tfrac{1}{2}$ y $b = t$:\n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty} e^{-ax^2 + bx}\\,dx = \\sqrt{\\frac{\\pi}{a}}\\,e^{\\frac{b^2}{4a}}\n",
    "$$\n",
    "Con $a = \\tfrac{1}{2}$, el factor $\\sqrt{\\pi/a} = \\sqrt{2\\pi}$ cancela la\n",
    "constante normalizadora, y el exponente resultante es $t^2/2$:\n",
    "$$\n",
    "M_X(t) = e^{\\,t^2/2}\n",
    "$$\n",
    "Verificamos los momentos mediante derivación. La primera derivada evaluada en\n",
    "$t=0$ confirma que la media es cero:\n",
    "$$\n",
    "\\left.\\frac{d}{dt}\\,e^{\\,t^2/2}\\right|_{t=0}\n",
    "= \\left. t\\,e^{\\,t^2/2} \\right|_{t=0} = 0 = E[X]\n",
    "$$\n",
    "La segunda derivada evaluada en $t=0$ confirma que la varianza es uno:\n",
    "$$\n",
    "E[X^2]\n",
    "= \\left.\\frac{d^2}{dt^2}\\,e^{\\,t^2/2}\\right|_{t=0}\n",
    "= \\left.\\left(e^{\\,t^2/2} + t^2 e^{\\,t^2/2}\\right)\\right|_{t=0} = 1\n",
    "$$\n",
    "Ambos resultados son consistentes con $X \\sim \\mathcal{N}(0,1)$.\n",
    "\n",
    "#### FGM de una combinación lineal de variables independientes.\n",
    "Sea $Y = c_1 X_1 + c_2 X_2 + \\cdots + c_n X_n$ con las $X_i$ independientes.\n",
    "Aplicando la definición y la independencia, la exponencial de la suma se\n",
    "factoriza en un producto de esperanzas:\n",
    "$$\n",
    "M_Y(t)\n",
    "= E\\!\\left[e^{t(c_1 X_1 + \\cdots + c_n X_n)}\\right]\n",
    "= E\\!\\left[e^{tc_1 X_1}\\right] E\\!\\left[e^{tc_2 X_2}\\right] \\cdots\n",
    "  E\\!\\left[e^{tc_n X_n}\\right]\n",
    "$$\n",
    "Reconociendo cada factor como la FGM de $X_i$ evaluada en $c_i t$:\n",
    "$$\n",
    "M_Y(t) = M_{X_1}(c_1 t)\\, M_{X_2}(c_2 t) \\cdots M_{X_n}(c_n t)\n",
    "$$\n",
    "Este resultado es clave: la FGM de una suma de variables independientes es el\n",
    "producto de sus FGM individuales, lo que simplifica enormemente el análisis de\n",
    "sumas aleatorias.\n",
    "\n",
    "### Teorema del Límite Central\n",
    "#### Enunciado. Sea $(X_i)_{i \\geq 1}$ una sucesión de variables\n",
    "aleatorias reales, independientes e idénticamente distribuidas (i.i.d.), con:\n",
    "$$\n",
    "E[X_i] = \\mu_X, \\qquad \\mathrm{Var}(X_i) = \\sigma_X^2 < \\infty\n",
    "$$\n",
    "Definimos la suma parcial y su versión estandarizada:\n",
    "$$\n",
    "Y_n = \\sum_{i=1}^n X_i\n",
    "$$\n",
    "Para comparar sumas de distinto tamaño, centramos restando la media $n\\mu_X$\n",
    "y escalamos dividiendo por la desviación estándar $\\sigma_X\\sqrt{n}$:\n",
    "$$\n",
    "Z_n\n",
    "= \\frac{Y_n - n\\mu_X}{\\sigma_X \\sqrt{n}}\n",
    "= \\frac{\\dfrac{1}{n}\\displaystyle\\sum_{i=1}^n (X_i - \\mu_X)}{\\sigma_X / \\sqrt{n}}\n",
    "$$\n",
    "Por construcción, $E[Z_n] = 0$ y $\\mathrm{Var}(Z_n) = 1$ para todo $n$.\n",
    "\n",
    "#### Demostración vía FGM.\n",
    "Asumimos que la FGM de $X_i$ existe y es finita en un entorno de $t = 0$.\n",
    "Introducimos las versiones estandarizadas individuales:\n",
    "$$\n",
    "U_i = \\frac{X_i - \\mu_X}{\\sigma_X}\n",
    "$$\n",
    "Por construcción estas variables tienen media cero y varianza uno:\n",
    "$$\n",
    "E[U_i] = 0, \\qquad E[U_i^2] = 1\n",
    "$$\n",
    "Observamos que $Z_n$ es simplemente la media de las $U_i$ reescalada:\n",
    "$$\n",
    "Z_n = \\frac{U_1 + U_2 + \\cdots + U_n}{\\sqrt{n}}\n",
    "$$\n",
    "Expandimos la FGM de $U_i$ en serie de Taylor alrededor de $t = 0$ hasta\n",
    "segundo orden, usando los momentos recién calculados:\n",
    "$$\n",
    "M_{U_i}(t)\n",
    "= 1 + t\\,E[U_i] + \\frac{t^2}{2}\\,E[U_i^2] + \\mathcal{O}(t^3)\n",
    "$$\n",
    "Sustituyendo $E[U_i] = 0$ y $E[U_i^2] = 1$, el término lineal desaparece:\n",
    "$$\n",
    "M_{U_i}(t) = 1 + \\frac{t^2}{2} + \\mathcal{O}(t^3)\n",
    "$$\n",
    "Usando la factorización de la FGM para sumas de variables independientes, y\n",
    "que todas las $U_i$ tienen la misma distribución:\n",
    "$$\n",
    "M_{Z_n}(t)\n",
    "= E\\!\\left[e^{\\,t(U_1 + \\cdots + U_n)/\\sqrt{n}}\\right]\n",
    "= \\prod_{i=1}^n E\\!\\left[e^{\\,t U_i/\\sqrt{n}}\\right]\n",
    "= \\left[M_U\\!\\left(\\frac{t}{\\sqrt{n}}\\right)\\right]^n\n",
    "$$\n",
    "Sustituyendo la expansión de $M_U$ con el argumento $t/\\sqrt{n}$, el término\n",
    "cúbico es de orden $n^{-3/2}$ y resulta despreciable:\n",
    "$$\n",
    "M_{Z_n}(t)\n",
    "= \\left[1 + \\frac{t^2}{2n} + \\mathcal{O}\\!\\left(\\frac{t^3}{n^{3/2}}\\right)\\right]^n\n",
    "$$\n",
    "Reconocemos la forma $\\lim_{n\\to\\infty}(1 + x/n)^n = e^x$. Al tomar el\n",
    "límite, el término de orden superior desaparece y obtenemos:\n",
    "$$\n",
    "\\lim_{n \\to \\infty} M_{Z_n}(t)\n",
    "= \\lim_{n \\to \\infty} \\left[1 + \\frac{t^2}{2n}\\right]^n\n",
    "= e^{\\,t^2/2}\n",
    "$$\n",
    "Este es exactamente el valor $M_X(t) = e^{t^2/2}$ que calculamos para la\n",
    "normal estándar. Dado que la FGM caracteriza unívocamente la distribución\n",
    "(cuando existe en un entorno de cero), concluimos que $Z_n$ converge en\n",
    "distribución a una $\\mathcal{N}(0,1)$:\n",
    "$$\n",
    "f_{Z_n}(z) \\;\\xrightarrow[n\\to\\infty]{\\;\\mathcal{D}\\;}\\;\n",
    "\\frac{1}{\\sqrt{2\\pi}}\\,e^{-z^2/2}\n",
    "$$\n",
    "Es decir, para $n$ suficientemente grande, la suma normalizada de variables\n",
    "i.i.d.\\ con media y varianza finitas se distribuye aproximadamente como una\n",
    "normal estándar, independientemente de la distribución original de las $X_i$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjQHVHOIm4k_"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNNU27Ycm9m/iEVGirB5c5W",
   "name": "sesion_00_introduccion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
